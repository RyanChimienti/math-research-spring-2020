\documentclass[12pt]{article}
\setlength\parindent{0pt}
\usepackage{mathtools}
\usepackage{amssymb,amsmath,amsthm,amsfonts}
\usepackage{thmtools}
\usepackage{cleveref}
\usepackage{changepage}
\usepackage{mathtools}
\usepackage[margin=0.75in]{geometry}
\setlength{\parindent}{1cm}

%omit eq. before equation references
\crefformat{equation}{(#2#1#3)}

%Allows me to use begin{thm}, begin{lem}, etc.%
\newtheorem{thm}{Theorem}
\crefname{thm}{Theorem}{Theorems}
\newtheorem{prop}[thm]{Proposition}
\crefname{prop}{Proposition}{Propositions}
\newtheorem{corollary}[thm]{Corollary}
\crefname{corollary}{Corollary}{Corollaries}
\newtheorem{lem}[thm]{Lemma}
\crefname{lem}{Lemma}{Lemmas}
\declaretheoremstyle[
	headfont=\normalfont\itshape , 
	headindent=\parindent]{casestyle}
\declaretheorem[
	style=casestyle ,
	numberwithin=thm]{case}
\renewcommand{\thecase}{\arabic{case}} %Suppress thm # before case #
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{rem}[thm]{Remark}

%Helpful Shortcuts%
\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\Q}{\mathbb Q}
\newcommand{\R}{\mathbb R}
\newcommand{\C}{\mathbb C}
\DeclarePairedDelimiter\V{\langle}{\rangle} 
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}

%Commands specifically for spectral graph theory
\DeclareMathOperator{\pr}{pr}
\DeclareMathOperator{\lazypr}{lazypr}
\DeclareMathOperator{\sigdist}{sigdist}
\DeclareMathOperator{\asd}{asd}
\DeclareMathOperator{\vol}{Vol}
\newcommand{\lap}{\mathcal{L}}
\newcommand{\normadj}{\mathcal{A}}
\newcommand{\laplace}{\Delta}
\newcommand{\green}{\mathcal{G}}
\newcommand{\asymgreen}{\normalfont{\textbf{G}}}
\DeclarePairedDelimiter\inner{\langle}{\rangle} 

%Allows me to make use of the \set* command, with a semicolon for the pipe.%
\usepackage{xparse}
%
\DeclarePairedDelimiterX{\set}[1]{\{}{\}}{\setargs{#1}}
\NewDocumentCommand{\setargs}{>{\SplitArgument{1}{;}}m}
{\setargsaux#1}
\NewDocumentCommand{\setargsaux}{mm}
{\IfNoValueTF{#2}{#1} {#1\,:\,\mathopen{}#2}}%{#1\:;\:#2}

\begin{document}

\section{The Magnetic Laplacian and its Spectrum}

\begin{defn}
A \textit{simple graph} is a set of vertices $V$ and a set of oriented edges $E \subseteq V \times V$, where for all $(v, w) \in E$, we have $v \neq w$ and $(w, v) \in E$.
\end{defn}

\begin{defn}
A \textit{magnetic graph} is a simple graph $(V, E)$ together with a \textit{signature} function $$\sigma : E \rightarrow S^1$$ which has the property $\sigma(v, w) = \overline{\sigma(w,v)}$ for all $(v, w) \in E$. We may abbreviate $\sigma(v, w)$ as $\sigma_{vw}$.
\end{defn}

We may sometimes treat a simple graph as a magnetic graph, in which case all signatures are assumed to be 1. Unless we explicitly state the vertex set of a graph, it is assumed to be $\set*{1, \dots, n}$ for some $n$.

\begin{defn}
Suppose $G=(V, E)$ is a magnetic graph with vertices $\set*{1, \dots, n}$. The \textit{Laplacian} of $G$ is the $n$ x $n$ matrix $(l_{ij})$ given by 
$$
l_{ij} =
\begin{cases}
d_i &\mbox{ if } i=j \\
-\sigma_{ij} &\mbox{ otherwise}
\end{cases}
$$
for all $i, j \in \set*{1 \dots n}.$
\end{defn}

Note that this ``magnetic" definition for a Laplacian matches the familiar definition of a Laplacian when the graph is simple. The Laplacian always has a nice spectrum to analyze, as the following theorem shows.

\begin{thm}\label{spectral thm applied to Laplacian}
If $G$ is a magnetic graph with Laplacian $L$, then there is an orthonormal basis of $\C^n$ consisting of eigenvectors of $L$, and the eigenvalues of $L$ are real.
\end{thm}
\begin{proof}
$L$ is Hermitian because $L = L^*$. Therefore, the Complex Spectral Theorem gives this statement exactly.
\end{proof}

We also want to establish that the eigenvalues of the Laplacian are non-negative. We will follow Jiang's paper (Theorem 3.5 and the preceding discussion), but modified slightly for magnetic graphs.

\begin{defn}
Let $G=(E,V)$ be a magnetic graph on $n$ vertices with Laplacian $L$. For an edge $(e, v)$ of $G$, the \textit{contribution} of $(e, v)$ to $L$ is the $n$ x $n$ matrix $(l_{ij})$ given by
$$
l_{ij} =
\begin{cases}
1 &\mbox{ if } i=j \mbox{ and } i \in \set*{e, v} \\
-\sigma_{ij} &\mbox{ if } \set*{i,j} = \set*{e,v} \\
0 &\mbox{ otherwise}
\end{cases}.
$$ 
We denote this matrix $L_{(e, v)}$.
\end{defn}

As the name ``contribution" suggests, the Laplacian of a graph is the sum of the contributions of its edges (only counting one edge between each pair of vertices):

\begin{prop}
If $G$ is a magnetic graph with Laplacian $L$, then 
$$
L = \sum_{\substack{(i, j) \in E \\ i < j}} L_{(i, j)}.
$$
\end{prop}

For the next few propositions, remember that if $z \in \C$, then $z \geq 0$ means $z$ is real and nonnegative. Also, $\cdot$ is the complex dot product.

\begin{prop}\label{contributions to Laplacian are positive semi-definite}
Let $G=(E, V)$ be a magnetic graph on $n$ vertices with Laplacian $L$, and let $(i, j) \in E$. Then for any vector $v \in \C^n$, we have 
$$v \cdot L_{(i,j)} v \geq 0.$$
\end{prop}
\begin{proof}
Let $v = (v_1, \dots, v_n)$. By direct computation, the vector $L_{(i,j)}v$ consists of all 0s, except for $v_i-\sigma_{ij}v_j$ in the $i$th slot and $v_j-\sigma_{ji}v_i$ in the $j$th slot. Therefore,
\begin{align*}
v \cdot L_{(i,j)} v &= v_i \overline{v_i-\sigma_{ij}v_j} + v_j \overline{v_j-\sigma_{ji}v_i} \\
&= v_i \left(\overline{v_i} - \overline{\sigma_{ij}}\overline{v_j}\right) + v_j \left(\overline{v_j} - \overline{\sigma_{ji}}\overline{v_i}\right) \\
&= v_i \overline{v_i} - v_i \overline{\sigma_{ij}}\overline{v_j} + v_j \overline{v_j} - v_j \overline{\sigma_{ji}}\overline{v_i} \\
&= v_i \overline{v_i} - v_i \overline{\sigma_{ij}}\overline{v_j} - \sigma_{ij}v_j \overline{v_i} + \sigma_{ij} v_j \overline{\sigma_{ij} v_j} \\
&= \left( v_i - \sigma_{ij} v_j \right) \overline{ \left( v_i - \sigma_{ij} v_j \right) } \\
&= \abs*{v_i - \sigma_{ij} v_j}^2 \\
&\geq 0.
\end{align*}
\end{proof}

\begin{prop}\label{Laplacian is positive semi-definite}
Let $G=(E, V)$ be a magnetic graph on $n$ vertices with Laplacian $L$. Then for any vector $v \in \C^n$, we have 
$$v \cdot L v \geq 0.$$
\end{prop}
\begin{proof}
We have:
$$v \cdot Lv 
= v \cdot \left(\sum_{\substack{(i, j) \in E \\ i < j}} L_{(i, j)}\right)v
= v \cdot \sum_{\substack{(i, j) \in E \\ i < j}} L_{(i, j)} v = \sum_{\substack{(i, j) \in E \\ i < j}} \left(v \cdot L_{(i, j)} v\right),
$$
where the last equality holds because inner products are additive in the second slot. By \cref{contributions to Laplacian are positive semi-definite}, this is a sum of nonnegative real numbers, so $v \cdot Lv \geq 0$.
\end{proof}

\begin{thm}
If $G$ is a magnetic graph with Laplacian $L$, then the eigenvalues of $L$ are nonnegative.
\end{thm}
\begin{proof}
Let $\lambda$ be an eigenvalue of $L$, and let $v \in \C^n$ be a nonzero eigenvector corresponding to $\lambda$. Then,
\begin{align*}
\lambda (v \cdot v) &= v \cdot \overline\lambda v \\
&= v \cdot \lambda v &&\text{(since $\lambda$ is real by \cref{spectral thm applied to Laplacian})} \\
&= v \cdot L v \\
&\geq 0 &&\text{(by \cref{Laplacian is positive semi-definite})}. \\
\end{align*}
But $v$ is nonzero, so $v \cdot v \geq 0$ by the definition of inner product. Thus we may conclude that $\lambda \geq 0$.
\end{proof}

\section{Renumbering Vertices}

The Laplacian of a magnetic graph is only defined when the graph's vertices are numbered $1, \dots, n$, and changing the numbering changes the Laplacian. However, we will show in this section that changing the vertex numbering has no effect on the spectrum of the Laplacian. This will allow us to unambiguously refer to the ``spectrum of the Laplacian" of any magnetic graph, even if its vertices are not numbered or we don't know how they're numbered. With that goal in mind, we will first need a few facts about similar matrices.

\begin{prop}\label{similar matrices have same nullity}
Suppose $A$ and $B$ are similar $n \times n$ matrices over $\C$. Then $A$ and $B$ have the same nullity.
\end{prop}
\begin{proof}
Since $A$ and $B$ are similar, there exists an invertible matrix $P \in M_{n \times n}(\C)$ such that $$B = P^{-1} A P.$$ Let $k$ be the nullity of $A$. Then there exists a basis $x_1, \dots, x_k$ of $\ker A$, where each $x_i \in \C^n$. To show that $k$ is also the nullity of $B$, we will show that 
$$P^{-1}x_1, \dots, P^{-1}x_k$$
is a basis for $\ker B$. First note that those vectors are linearly independent, since $x_1, \dots, x_k$ are linearly independent, and an invertible linear map preserves linear independence. To see that $\text{span} (P^{-1}x_1, \dots, P^{-1}x_k) \subseteq \ker B$, observe that for any $i \in \set*{1, \dots, k}$, we have
\begin{align*}
B(P^{-1}x_i)
&= (P^{-1}AP)P^{-1}x_i \\
&= P^{-1} A x_i \\
&= P^{-1} (0) \\
&= 0,
\end{align*}
so $P^{-1}x_i \in \ker B$. It remains to show that $\ker B \subseteq \text{span} (P^{-1}x_1, \dots, P^{-1}x_k)$. To that end, let $v \in \ker B$. Then,
\begin{align*}
P^{-1}AP v = B v = 0 \\
&\rightarrow AP v = 0 \\
&\rightarrow Pv \in \ker A \\
&\rightarrow Pv = c_1 x_1 + \dots + c_k x_k &&\text{for some $c_1, \dots, c_k \in \C$} \\
&\rightarrow v = c_1 P^{-1} x_1 + \dots + c_k P^{-1} x_k &&\text{for some $c_1, \dots, c_k \in \C$} \\
&\rightarrow v \in \text{span} (P^{-1}x_1, \dots, P^{-1}x_k).
\end{align*} 
\end{proof}

\begin{prop}\label{similar matrices have same spectrum}
Suppose $A$ and $B$ are similar $n \times n$ matrices over $\C$. Then $A$ and $B$ have the same eigenvalues, counting multiplicity.
\end{prop}
\begin{proof}
Given $\lambda \in \C$ and $k \in \N$, we must show that $\lambda$ is an eigenvalue of $A$ with multiplicity $k$ if and only if $\lambda$ is an eigenvalue of $B$ with multiplicity $k$. The directions are analogous, so we will only show the forward direction. Suppose $\lambda$ is an eigenvalue of $A$ with multiplicity $k$. Since $A$ and $B$ are similar, there exists an invertible matrix $P \in M_{n \times n}(\C)$ such that 
$$B = P^{-1} A P.$$
Then, we have
\begin{align*}
\dim \ker (B - \lambda I) 
&= \dim \ker (P^{-1} A P - \lambda I) \\
&= \dim \ker (P^{-1} (A - \lambda I) P) \\
&= \dim \ker (A - \lambda I) &&\text{(by \cref{similar matrices have same nullity})} \\
&= k.
\end{align*}
Therefore, $\lambda$ is an eigenvalue of $B$ with multiplicity $k$.
\end{proof}

\begin{prop}\label{permuted matrices are similar}
Let $\phi$ be a permutation of $\set*{1, \dots, n}$, and let $A, B \in M_{n \times n}(\C)$ be related by 
$$A_{ij} = B_{\phi(i)\phi(j)}$$
for all $i, j \in \set*{1, \dots, n}$. Then
$$A=P^{-1}BP$$
for some permutation matrix $P \in M_{n \times n}(\C)$.
\end{prop}
\begin{proof}
Define $P$ by 
$$
P_{ij} = 
\begin{cases}
1 &\mbox{ if } i=\phi(j) \\
0 &\mbox{ otherwise}
\end{cases}.
$$
Since $P$ is a permutation matrix, its inverse is its transpose, so the elements of $P^{-1}$ are given by
$$
P^{-1}_{ij} = 
P_{ji} =
\begin{cases}
1 &\mbox{ if } j=\phi(i) \\
0 &\mbox{ otherwise}
\end{cases}.
$$
It can now be verified by computation that for all $i, j \in \set*{1, \dots, n}$, we have
$$\left( P^{-1}BP \right)_{ij} = B_{\phi(i)\phi(j)} = A_{ij}.$$
\end{proof}

Finally we can prove the main theorem of this section:

\begin{thm}\label{renumbering vertices doesn't change Laplacian spectrum}
Let $G$ be a magnetic graph with vertices $\set*{1, \dots, n}$. Then renumbering the vertices of $G$ does not change the spectrum of its Laplacian.
\end{thm}
\begin{proof}
Let $L$ be the Laplacian of $G$. Renumbering the vertices of $G$ means selecting a permutation $\phi$ of $\set*{1, \dots, n}$ and replacing each vertex $v$ with $\phi(v)$. This results in a new Laplacian $L^\prime$, where clearly $L^\prime_{ij} = L_{\phi^{-1}(i)\phi^{-1}(j)}$ for all $i, j$. Applying \cref{permuted matrices are similar}, we get that 
$$L^\prime_{ij}=P^{-1}LP$$
for some permutation matrix $P$. Finally, \cref{similar matrices have same spectrum} gives that $L^\prime$ has the same spectrum as $L$.  
\end{proof}

As an example of the importance of \cref{renumbering vertices doesn't change Laplacian spectrum}, the following theorem would be confusing without it. In this theorem, we can assume that the vertices of $G$ are the numbers $1, \dots, n$, so we know exactly what matrix is meant by the Laplacian $L$ of $G$. However, if the vertices of $G$ are indeed $1, \dots, n$, then it isn't possible for all the connected components to also have vertices in that format (only one of the components could get the vertex 1, for example). Therefore, the Laplacians $L_1, \dots, L_k$ are ambiguous. The key is to remember that every numbering for the vertices of a component gives a Laplacian with the same spectrum, so it doesn't matter which one we choose. $L_1, \dots, L_k$ are not really \textit{the} Laplacians of $G_1, \dots, G_k$, but rather some Laplacians that result from an arbitrary choice of vertex numberings. And the theorem holds regardless of the choice. 

It might simplify the numbering considerations if we define the \textit{Laplacian spectrum} of a graph, which would be the unique spectrum resulting from every numbering of the vertices. The Laplacian spectrum would then be independent of the choice of numbering, meaning we could often ignore numberings entirely, or just choose them as we need them. But I'm not sure that's worth creating a new definition.

\begin{thm}\label{spectrum is sum of spectra of connected components}
Let $G$ be a magnetic graph with Laplacian $L$, and let $G_1, \dots G_k$ be the connected components of $G$, with Laplacians $L_1, \dots, L_k$. Then the spectrum of $L$ is equal to the joined spectra of $L_1, \dots, L_k$.  
\end{thm}
\begin{proof}
Renumber the vertices of $G$ so that all the vertices of $G_1$ precede all the vertices of $G_2$, and so on. By \cref{renumbering vertices doesn't change Laplacian spectrum}, this doesn't affect the spectrum of $L$. But it makes $L$ into a block diagonal matrix whose blocks $B_1, \dots, B_k$ are some Laplacians of $G_1, \dots, G_k$, respectively. Since $B_1, \dots, B_k$ are Laplacians of $G_1, \dots, G_k$, \cref{renumbering vertices doesn't change Laplacian spectrum} says they have the same spectra as $L_1, \dots, L_k$. Therefore, it suffices to prove the spectrum of $L$ is equal to the joined spectra of $B_1, \dots, B_k$.

Let $\lambda \in \C$, and let $m_1, \dots, m_k$ be the multiplicities of $\lambda$ as an eigenvalue of $B_1, \dots, B_k$, respectively. We will be done if we can show that the multiplicity of $\lambda$ as an eigenvalue of $L$ is equal to $m_1 + \dots + m_k$. Indeed, the multiplicity of $\lambda$ as an eigenvalue of $L$ is
\begin{align*}
\dim \ker (L - \lambda I)
&= \dim \ker \left(
\begin{pmatrix}
B_1 &  &  & 0 \\
 & B_2 &  &  \\
  &   & \ddots &  \\
0 &  &  & B_k 
\end{pmatrix} 
- \lambda I
\right) \\
&= \dim \ker
\begin{pmatrix}
B_1 - \lambda I &  &  & 0 \\
 & B_2 - \lambda I &  &  \\
  &   & \ddots &  \\
0 &  &  & B_k - \lambda I
\end{pmatrix} \\
&= \dim \ker (B_1 - \lambda I) + \dots + \dim \ker (B_k - \lambda I) \\
&= m_1 + \dots + m_k.
\end{align*}
Here, the second to last equality holds because the nullity of a block diagonal matrix is equal to the sum of the nullities of the blocks (to see this, prove the analogous statement for rank, apply the rank nullity theorem, and simplify).
\end{proof}

\section{Balanced Magnetic Graphs}

\begin{defn}
A magnetic graph is called \textit{balanced} if the signatures along any closed walk multiply to 1.
\end{defn}

\begin{prop}\label{equivalent conditions for balancedness}
Let $G=(E, V, \sigma)$ be a magnetic graph. Then the following are equivalent.
\begin{enumerate}
\item G is balanced.
\item The signatures along any cycle multiply to 1 (in either direction).
\item For every pair of connected vertices $u, v \in V$, there exists $z \in S^1$ such that along every walk from $u$ to $v$, the signatures multiply to $z$. 
\end{enumerate}
\end{prop}

Because the number $z$ in condition 3 is clearly unique for each pair of connected vertices, we can make the following definition.

\begin{defn}
Let $G=(E,V, \sigma)$ be a balanced magnetic graph, and let $u, v \in V$ be connected. Then the \textit{signature distance} from $u$ to $v$, written sigdist$(u, v)$, is the product of the signatures along every walk from $u$ to $v$.
\end{defn}

\section{Switching Equivalence}

\begin{defn}
If $G=(V, E, \sigma)$ is a magnetic graph, then a function $c: V \rightarrow S^1$ is called a \textit{switching function} for $G$. We may write $c_v$ as shorthand for $c(v)$.
\end{defn}

\begin{defn}
Let $G = (V, E, \sigma)$ be a magnetic graph, and let $c$ be a switching function for $G$. Then $c$ \textit{applied to $G$} is the magnetic graph with the same edges and vertices, whose signature function $\tau$ is given by
$$\tau_{vw} = \overline{c_v}\sigma_{vw}c_w$$
for all $v \sim w$.
\end{defn}

\begin{defn}
A magnetic graph $G$ is \textit{switching equivalent} to a magnetic graph $H$ if there exists a switching function $c$ for $G$ such that $c$ applied to $G$ gives $H$.   
\end{defn}

\begin{prop}
Switching equivalence is an equivalence relation.
\end{prop}

\begin{prop}\label{switching equivalence of connected components}
Let $G^\sigma = (V, E, \sigma)$ and $G^\tau = (V, E, \tau)$ be two magnetic graphs with the same vertices and edges. Then $G^\sigma$ and $G^\tau$ are switching equivalent if and only if each connected component of $G^\sigma$ is switching equivalent to the corresponding connected component of $G^\tau$.
\end{prop}

\begin{proof}
$(\rightarrow)$ Suppose $G^\sigma$ and $G^\tau$ are switching equivalent. Then there is a switching function $c$ for $G^\sigma$ that, when applied to $G^\sigma$, gives $G^\tau$. Now let $C^\sigma=(U, D, \sigma|_{D})$ be an arbitrary connected component of $G^\sigma$. Then $C^\tau=(U, D, \tau|_{D})$ is the corresponding connected component of $G^\tau$. Choose $b$ to be the restriction of $c$ to $U$, and note that $b$ is a switching function for $C^\sigma$. It remains to show that $b$ applied to $C^\sigma$ gives $C^\tau$. Indeed, for any adjacent vertices $v, w \in U$, the signature of $(v, w)$ given by $b$ applied to $C^\sigma$ is
$$
\overline{b_v} (\sigma|_D)_{vw} b_w 
= \overline{c_v} \sigma_{vw} c_w
= \tau_{vw}
= (\tau|_{D})_{vw},
$$
which is the signature of $(v, w)$ in $C^\tau$.

$(\leftarrow)$ Suppose each connected component of $G^\sigma$ is switching equivalent to the corresponding connected component of $G^\tau$. Call the connected components $C_1, \dots, C_k.$ Then, for each $i \in \set*{1, \dots, k}$, let $b^i$ be the switching function that takes the $i$th connected component of $G^\sigma$ to the $i$th connected component of $G^\tau$. Define a switching function $c$ for $G^\sigma$ by $c_v = (b^i)_v$, where $i$ is such that $v$ belongs to $C_i$. It is easy to verify that $c$ applied to $G^\sigma$ gives $G^\tau$.
\end{proof}

We now work towards another characterization of switching equivalence. We want to show that two magnetic graphs with the same vertices and edges are switching equivalent if and only if the product of the signatures along any closed walk is the same for the two graphs. To make this easier, we will start with a lemma.

\begin{lem}\label{balanced implies switching equivalent to simple graph}
If $G=(V, E, \sigma)$ is a balanced magnetic graph, then $G$ is switching equivalent to the simple graph on $(V, E)$.
\end{lem}
\begin{proof}
Because of \cref{switching equivalence of connected components} and the fact that connected components of a balanced graph are also balanced, it suffices to prove the lemma in the case where $V$ is connected. So assume $V$ is connected. We must construct a switching function that takes every signature in $G$ to 1. To do this, fix a vertex $u_0 \in V$ and define the switching function $c: V \rightarrow \C_{|z|=1}$ by 
$$c_u = \text{sigdist} (u, u_0)$$
for all $u \in V$. Then for every pair of adjacent vertices $v, w \in V$, we have
\begin{align*}
\overline{c_v} \sigma_{vw} c_w
&= \overline{\text{sigdist} (v, u_0)} \sigma_{vw} \text{sigdist} (w, u_0) \\
&= \overline{\text{sigdist} (v, u_0)} \text{sigdist} (v, u_0) \\
&= 1,
\end{align*}
as desired.
\end{proof}

\begin{thm}\label{closed walk characterization of switching equivalence}
Let $G^\sigma = (V, E, \sigma)$ and $G^\tau = (V, E, \tau)$ be two magnetic graphs with the same vertices and edges. Then $G^\sigma$ and $G^\tau$ are switching equivalent if and only if along every closed walk, the product of the signatures of $G^\sigma$ equals the product of the signatures of $G^\tau$.
\end{thm}
\begin{proof}
$(\rightarrow)$ Suppose $G^\sigma$ and $G^\tau$ are switching equivalent. Then there is a switching function $c: V \rightarrow S^1$ that takes $G^\sigma$ to $G^\tau$. Let $v_1, \dots, v_k$ be the vertices along a closed walk, so that $v_1 = v_k$. Then,
\begin{align*}
\sigma_{v_1 v_2} \sigma_{v_2 v_3} \dots \sigma_{v_{k-1} v_k}
&= \overline{c_{v_1}} \sigma_{v_1 v_2} \sigma_{v_2 v_3} \dots \sigma_{v_{k-1} v_k} c_{v_1} \\
&= \overline{c_{v_1}} \sigma_{v_1 v_2} \sigma_{v_2 v_3} \dots \sigma_{v_{k-1} v_k} c_{v_k} \\
&= (\overline{c_{v_1}} \sigma_{v_1 v_2} c_{v_2}) (\overline{c_{v_2}} \sigma_{v_2 v_3} c_{v_3}) \dots (\overline{c_{v_{k-1}}} \sigma_{v_{k-1} v_k} c_{v_k}) \\
&= \tau_{v_1 v_2} \tau_{v_2 v_3} \dots \tau_{v_{k-1} v_k}.
\end{align*} 
Therefore, along every closed walk, the product of the signatures of $G^\sigma$ equals the product of the signatures of $G^\tau$.

$(\leftarrow)$ Suppose that along every closed walk, the product of the signatures of $G^\sigma$ equals the product of the signatures of $G^\tau$. Define a new graph $G^s = (V, E, s)$, where the signatures $s$ are given by 
$$s_{vw} = \overline{\tau_{vw}} \sigma_{vw}$$ 
for all vertices $v \sim w$. It is easy to see that along any closed walk, the signatures of $G^s$ multiply to 1. Therefore, $G^s$ is balanced. By \cref{balanced implies switching equivalent to simple graph}, it follows that $G^s$ is switching equivalent to the simple graph on $(V, E)$, whose signatures are all 1. Call the switching function that takes $G^s$ to the simple graph $c$. Then for any vertices $v \sim w$, we have
\begin{align*}
\overline{c_v} s_{vw} c_w = 1
&\rightarrow \overline{c_v} \left(\overline{\tau_{vw}} \sigma_{vw}\right) c_w = 1 \\
&\rightarrow \overline{c_v} \sigma_{vw} c_w = \tau_{vw}.
\end{align*}
Therefore, applying the switching function $c$ to $G^\sigma$ gives $G^\tau$, so $G^\sigma$ and $G^\tau$ are switching equivalent.
\end{proof}

Many nice consequences flow from this characterization of switching equivalence. 

\begin{corollary}\label{balanced iff switching equivalent to simple graph}
A magnetic graph is balanced if and only if it is switching equivalent to the simple graph with the same vertices and edges.
\end{corollary}

\begin{corollary}
If a magnetic graph is switching equivalent to a balanced magnetic graph, then it is also balanced.  
\end{corollary}

\begin{corollary}
Any two balanced magnetic graphs with the same vertices and edges are switching equivalent. 
\end{corollary}

\begin{corollary}
Every acyclic magnetic graph is switching equivalent to the simple graph.
\end{corollary}

\begin{corollary}
Every magnetic cycle graph is switching equivalent to a graph which differs from the simple graph by at most one edge. 
\end{corollary}

The next theorem is why we care about switching equivalence in the context of spectral graph theory.

\begin{thm}\label{switching preserves spectrum}
Let $G^\sigma = (V, E, \sigma)$ and $G^\tau = (V, E, \tau)$ be magnetic graphs with the same vertices and edges. Then if $G^\sigma$ and $G^\tau$ are switching equivalent, it follows that their Laplacians have the same spectrum.
\end{thm}
\begin{proof}
Let $n$ be the number of vertices in $V$. Let $L^\sigma$ and $L^\tau$ be the Laplacians for $G^\sigma$ and $G^\tau$. Let $c$ be a switching function that takes $G^\sigma$ to $G^\tau$. Then, 
\begin{align*}
L^\tau &= \text{diag}(\overline{c_1}, \dots, \overline{c_n})\left(L^\sigma\right) \text{diag}(c_1, \dots, c_n) \\
&= \text{diag}(c_1, \dots, c_n)^{-1} \left(L^\sigma\right) \text{diag}(c_1, \dots, c_n).
\end{align*}
We see that $L^\sigma$ and $L^\tau$ are similar matrices. Therefore, by \cref{similar matrices have same spectrum}, they have the same spectrum.
\end{proof}

The converse of \cref{switching preserves spectrum} doesn't hold. For example, the 3-cycle graph with signatures 1, 1, and $i$ (going in order around the cycle) has the same spectrum as the 3-cycle graph with signatures 1, 1, and $-i$. However, the product along the cycles is different, so by \cref{closed walk characterization of switching equivalence}, the graphs cannot be switching equivalent. 

\section{The Eigenvalue 0 of the Laplacian}

If $G$ is a magnetic graph with vertices $\set*{1, \dots, n}$ and $x$ is a vector in $\C^n$, then it makes sense to imagine $x$ as consisting of values for the vertices of $G$. Specifically, the entry $x_i$ is like a value for the vertex $i$. With this interpretation, multiplying $x$ by the Laplacian of $G$ creates a new list of vertex values by taking linear combinations of the current vertex values. The weights of the combinations are determined by the structure of the Laplacian. In fact, the next proposition shows that a vertex's new value after the multiplication depends only on its current value and the values of its neighbors.   


\begin{prop}\label{Laplacian times vector}
Let $G$ be a magnetic graph on $n$ vertices with Laplacian $L$. Let $x \in \C^n$ with $x = (x_1, \dots, x_n)$. Then
$$(Lx)_i = d_ix_i - \sum_{j \sim i} \sigma_{ij} x_j$$
for each $i \in \set*{1, \dots, n}$.
\end{prop}
\begin{proof}
Let $l_{ij}$ denote the element of $L$ at row $i$ and column $j$. Let $i \in \set*{1, \dots, n}$ be given. Then,
\begin{align*}
(Lx)_i &= \sum_{j=1}^n l_{ij}x_j \\
&= l_{ii}x_i + \sum_{j \neq i} l_{ij}x_j \\
&= d_i x_i + \sum_{j \sim i} (-\sigma_{ij})x_j \\
&= d_i x_i - \sum_{j \sim i} \sigma_{ij}x_j.
\end{align*}
\end{proof}

We are particularly interested in when the ``vertex values" all go to 0 under multiplication by the Laplacian. It turns out that this happens exactly when each value is a signature-weighted average of the neighboring values, as the next lemma demonstrates.

\begin{lem}\label{entries of vector in Laplacian's null space are signature-weighted averages}
Let $G$ be a magnetic graph on $n$ vertices with Laplacian $L$. Suppose $x \in \C^n$ with $x = (x_1, \dots, x_n)$. Then $Lx = 0$ if and only if
$$x_i = \frac{1}{d_i} \sum_{j \sim i} \sigma_{ij}x_j$$
for each $i \in \set*{1, \dots n}$ with $d_i \neq 0$.
\end{lem}
\begin{proof}
This follows easily from \cref{Laplacian times vector}.
\end{proof}

We now show that this condition is equivalent to a much simpler one.

\begin{thm}\label{characterization of null space of Laplacian}
Let $G$ be a magnetic graph on $n$ vertices with Laplacian $L$. Suppose $x \in \C^n$ with $x = (x_1, \dots, x_n)$. Then $Lx = 0$ if and only if
$$x_i = \sigma_{ij}x_j$$
whenever $i \sim j$.
\end{thm}
\begin{proof}
$(\leftarrow)$ Suppose $x_i = \sigma_{ij}x_j$ whenever $i \sim j$. We will show that $Lx = 0$ using the condition from \cref{entries of vector in Laplacian's null space are signature-weighted averages}. Fix $i \in \set*{1, \dots, n}$ with $d_i \neq 0$. Then,
$$
x_i 
= \frac{1}{d_i} d_i x_i
= \frac{1}{d_i} \sum_{j \sim i} x_i
= \frac{1}{d_i} \sum_{j \sim i} \sigma_{ij}x_j.
$$ 

$(\rightarrow)$ Suppose $Lx = 0$. We first argue that whenever $a$ and $b$ are vertices in the same connected component of $G$, we have $\abs*{x_a} = \abs*{x_b}$. To see this, take an arbitrary connected component of $G$, and choose a vertex $m$ in that component which maximizes $\abs*{x_m}$. If $d_m = 0$, then $m$ is the only vertex in the component, so our conclusion holds trivially. On the other hand, if $d_m \neq 0$, then we can apply \cref{entries of vector in Laplacian's null space are signature-weighted averages} to get
\begin{align*}
\abs*{x_m} &= \abs*{\frac{1}{d_m} \sum_{l \sim m} \sigma_{ml} x_l} \\
&= \frac{1}{d_m} \abs*{\sum_{l \sim m} \sigma_{ml} x_l} \\
&\leq \frac{1}{d_m} \sum_{l \sim m} \abs*{\sigma_{ml} x_l} && \text{(by the Triangle Inequality)} \\
&= \frac{1}{d_m} \sum_{l \sim m} \abs*{x_l}.
\end{align*}
That is, $\abs*{x_m}$ is the mean of the values $\abs{x_l}$ where $l \sim m$. But our choice of $m$ ensures that $\abs*{x_m} \geq \abs*{x_l}$ for each $l \sim m$, so in fact $\abs*{x_m} = \abs*{x_l}$ for each $l \sim m$. It follows that any vertex $l$ which is adjacent to $m$ also maximizes the quantity $\abs*{x_l}$, so we can apply the same argument to each $l$, repeating the process until we conclude that $\abs*{x_a} = \abs*{x_b}$ for every pair of vertices $a, b$ in the connected component.

Now let $i$ and $j$ be vertices of $G$ with $i \sim j$. We want to prove that $x_i = \sigma_{ij}x_j$. From the previous paragraph, we already have that
$$\abs*{x_i} = \abs*{x_j} = \abs*{\sigma_{ij} x_j},$$
so it suffices to show $x_i$ differs from $\sigma_{ij} x_j$ by a positive real factor. Observe that 
\begin{align*}
\abs*{\sum_{l \sim i} \sigma_{il}x_l} &= \abs*{d_i x_i} && \text{(by \cref{entries of vector in Laplacian's null space are signature-weighted averages})} \\
&= d_i \abs*{x_i} \\
&= \sum_{l \sim i} \abs*{x_i} \\
&= \sum_{l \sim i} \abs*{x_l} && \text{(by the previous paragraph)} \\
&= \sum_{l \sim i} \abs*{\sigma_{il} x_l}.
\end{align*}
Therefore, assuming $\sigma_{ij} x_j \neq 0$ (the other case is trivial), we may conclude that 
$$\sum_{l \sim i} \sigma_{il}x_l = r (\sigma_{ij}x_j)$$
for some positive real $r$. Then,
\begin{align*}
x_i &= \frac{1}{d_i} \sum_{l \sim i} \sigma_{il}x_l && \text{(by \cref{entries of vector in Laplacian's null space are signature-weighted averages})} \\
&= \frac{1}{d_i} r (\sigma_{ij}x_j),
\end{align*}
so $x_i$ differs from $\sigma_{ij}x_j$ by a positive real factor, as desired.
\end{proof}

\begin{corollary}\label{null space of simple graph Laplacian}
Let $G$ be a simple graph on $n$ vertices with Laplacian $L$. Suppose $x \in \C^n$ with $x = (x_1, \dots, x_n)$. Then $Lx = 0$ if and only if $$x_i = x_j$$ whenever $i \sim j$.
\end{corollary}
\begin{proof}
This follows from \cref{characterization of null space of Laplacian} and the fact that every signature in a simple graph is 1.
\end{proof}

In a way, \cref{characterization of null space of Laplacian} completely answers the question of when 0 is an eigenvalue of the Laplacian. 0 is an eigenvalue exactly when there is a nonzero vector $x$ satisfying the condition in \cref{characterization of null space of Laplacian}. But that condition is phrased in terms of local properties of the graph. We want to find global properties that determine whether 0 is an eigenvalue, and if so, reveal its multiplicity. The final few results of the section achieve that.

\begin{prop}\label{multiplicity of 0 for connected balanced graph}
If $G$ is a connected and balanced magnetic graph, then 0 is an eigenvalue of its Laplacian with multiplicity 1. 
\end{prop}
\begin{proof}
Since $G$ is balanced, \cref{balanced iff switching equivalent to simple graph} gives that it is switching equivalent to the simple graph on the same edges and vertices. By \cref{switching preserves spectrum}, the spectrum of that simple graph is the same as the spectrum of $G$. So we only have to show that 0 is an eigenvalue with multiplicity 1 for the Laplacian of a connected, simple graph. Indeed, it is clear from \cref{null space of simple graph Laplacian} that the kernel of such a Laplacian is exactly the one-dimensional space of vectors whose entries are all the same.   
\end{proof}

\begin{prop}\label{multiplicity of 0 for connected unbalanced graph}
If $G$ is a connected and unbalanced magnetic graph, then 0 is not an eigenvalue of its Laplacian. 
\end{prop}
\begin{proof}
Let $n$ be the number of vertices in $G$, and let $x = (x_1, \dots, x_n) \in \ker L$. We will be done if we can show that $x$ must be $0$. Since $G$ is unbalanced, it has a closed walk along which the signatures multiply to some $c \neq 1$. Let $k$ be the starting and ending vertex of the walk. Repeated application of \cref{characterization of null space of Laplacian} along that walk gives that $x_k = c x_k$, so $x_k = 0$. Then applying \cref{characterization of null space of Laplacian} outward from $k$, we get $x_j = 0$ for every $j \in \set*{1, \dots, n}$. This means $x = 0$, as desired.
\end{proof}

\begin{thm}\label{nullity of Laplacian}
Let $G$ be a magnetic graph with Laplacian $L$. Then the multiplicity of 0 as an eigenvalue of $L$ is equal to the number of balanced connected components of $G$.
\end{thm}
\begin{proof}
By \cref{spectrum is sum of spectra of connected components}, it suffices to sum the multiplicities of 0 as an eigenvalue for $G$'s connected components. By \cref{multiplicity of 0 for connected balanced graph}, the balanced components each contribute 1 multiplicity, and by \cref{multiplicity of 0 for connected unbalanced graph}, the unbalanced components each contribute 0 multiplicity. Therefore, the total multiplicity is the number of balanced components.
\end{proof}

\begin{corollary}
The Laplacian of a simple graph always has 0 as an eigenvalue, and its multiplicity is equal to the number of connected components in the graph.
\end{corollary}
\begin{proof}
A simple graph is balanced because all its signatures are 1. Thus all its connected components are balanced. Since the multiplicity of 0 is equal to the number of balanced components, in this case it is just equal to the total number of components.
\end{proof}

\section{The Frustration Index}
The frustration index of a magnetic graph is a way to measure how close the graph is to being balanced.

\begin{defn}\label{defn: frustration index}
Let $G = (V, E, \sigma)$ be a magnetic graph. Then the frustration index of $G$, denoted $\iota(G)$, is 
$$
\min_{c: V \rightarrow S^1}
\sum_{i \sim j} 
\abs*{\overline{c_i} \sigma_{ij} c_j - 1}.
$$
\end{defn}

Note that the minimum exists by a compactness argument. A balanced graph has frustration index 0 (since it is switching equivalent to a graph whose signatures are all 1). Cycle graphs also have a nice frustration index, as we will show next. For this we need a lemma.

\begin{lem}\label{lemma for cycle frustration}
If $z_1, \dots, z_n \in S^1$, then
$$
\abs*{z_1 z_2 \dots z_n - 1} \leq \abs*{z_1 - 1} + \dots + \abs*{z_n - 1}.
$$
\end{lem}
\begin{proof}
We use induction on $n$. The claim is clearly true in the base case $n = 1$. Now let $k \in \N$, and assume the claim holds for $n = k$. Then,
\begin{align*}
\abs*{z_1 z_2 \dots z_{k+1} - 1}
&= \abs*{z_1 z_2 \dots z_{k+1} - z_{k+1} + z_{k+1} - 1} \\
&\leq \abs*{z_1 z_2 \dots z_{k+1} - z_{k+1}} + \abs*{z_{k+1} - 1}\\
&= \abs*{z_{k+1}(z_1 z_2 \dots z_{k} - 1)} + \abs*{z_{k+1} - 1} \\
&= \abs*{z_{k+1}} \abs*{z_1 z_2 \dots z_{k} - 1} + \abs*{z_{k+1} - 1} \\
&= \abs*{z_1 z_2 \dots z_{k} - 1} + \abs*{z_{k+1} - 1} \\
&\leq \abs*{z_1 - 1} + \dots + \abs*{z_k - 1} + \abs*{z_{k+1} - 1}.
\end{align*}
So the claim also holds for $n = k + 1$, and we are done.
\end{proof}

\begin{prop}\label{frustration of cycle graph}
Let $G = (V, E, \sigma)$ be a cycle graph on the vertices $\set*{1, \dots, n}$. Let $p$ be the product of the signatures along the cycle in one direction. Then the frustration index of $G$ is $2 \abs*{p - 1}$.
\end{prop}
\begin{proof}
Without loss of generality, let 
$$p=\sigma_{12}\sigma_{23} \dots \sigma_{n1}.$$
By \cref{closed walk characterization of switching equivalence}, $G$ is switching equivalent to the graph with signatures $\tau$, where 
\begin{align*}
\tau_{12} &= \tau_{23} = \dots = \tau_{(n-1)n} = 1, \text{ and} \\
\tau_{n1} &= p.
\end{align*}
Let $b$ be a switching function which takes $G$'s signatures to $\tau$. Then,
\begin{align*}
\sum_{i \sim j} \abs*{\overline{b_i} \sigma_{ij} b_j - 1} 
&= \sum_{i \sim j} \abs*{\tau_{ij} - 1} \\
&= \abs*{\tau_{n1} - 1} + \abs*{\tau_{1n} - 1} && \text{(eliminating 0 terms)} \\
&= \abs*{p - 1} + \abs*{\overline{p} - 1} \\
&= \abs*{p - 1} + \abs*{p - 1} \\
&= 2 \abs*{p - 1}.
\end{align*}
So we have shown that there exists a switching function $b$ for $G$ with 
$$
\sum_{i \sim j} \abs*{\overline{b_i} \sigma_{ij} b_j - 1} = 2 \abs*{p - 1}.
$$ 
To finish the proof, we must show that whenever $c$ is a switching function for $G$, we have
$$
\sum_{i \sim j} \abs*{\overline{c_i} \sigma_{ij} c_j - 1} \geq 2 \abs*{p - 1}.
$$
To see this, let $c$ be an arbitrary switching function for $G$, and let $s$ be the new signatures obtained by applying $c$ to $G$. Then,
\begin{alignat*}{3}
&& &\sum_{i \sim j} \abs*{\overline{c_i} \sigma_{ij} c_j - 1} && \\
&= & \quad &\sum_{i \sim j} \abs*{s_{ij} - 1} && \\
&= & &2 \left( \abs*{s_{12} - 1} + \abs*{s_{23} - 1} + \dots + \abs*{s_{n1} - 1} \right) && \\
&\geq & &2 \abs*{s_{12} s_{23} \dots s_{n1} - 1} & &\text{(\cref{lemma for cycle frustration})} \\
&= & &2 \abs*{\sigma_{12} \sigma_{23} \dots \sigma_{n1} - 1} &\quad\quad &\text{(switching preserves products along cycles)} \\
&= & &2 \abs*{p-1}. &&
\end{alignat*}
\end{proof}

Actually, we can generalize \cref{frustration of cycle graph} to arbitrary magnetic graphs. We start with a lemma.

\begin{lem}\label{lemma for frustration spanning tree thm}
Let $g : \R \rightarrow \R$ be given by $g(\theta) := \sqrt{2 - 2 \cos \theta}.$ Then $g^{\prime \prime}$ is defined on the set $\R \setminus \set*{2 k \pi : k \in \Z}$, and it is negative everywhere.
\end{lem}
\begin{proof}
It suffices to prove the same statement for the function $h(\theta) := \sqrt{1 - \cos \theta}$. By the chain rule, $h^\prime$ is defined on $\R \setminus \set*{2 k \pi : k \in \Z}$ and is given by 
$$h^\prime(\theta) = \frac{\sin \theta}{2 \sqrt{1 - \cos \theta}}.$$   
Now, by the quotient rule, $h^{\prime \prime}$ is defined on $\R \setminus \set*{2 k \pi : k \in \Z}$ and is given by
\begin{align*}
h^{\prime \prime}(\theta) 
&= \frac{\left( 2 \sqrt{1 - \cos \theta} \right) ( \cos \theta ) - (\sin \theta) \left( \frac{\sin \theta}{\sqrt{1 - \cos \theta}} \right)}{4(1 - \cos \theta)} \\
&= \frac{2(1 - \cos \theta) \cos \theta - \sin^2 \theta}{4(1 - \cos \theta)^{3/2}} \\
&= \frac{2 \cos \theta - 2 \cos^2 \theta - \sin^2 \theta}{4(1 - \cos \theta)^{3/2}} \\
&= - \frac{\cos^2 \theta - 2 \cos \theta + 1}{4(1 - \cos \theta)^{3/2}} \\
&= - \frac{(1 - \cos \theta)^2}{4(1 - \cos \theta)^{3/2}} \\
&= - \frac{1}{4} \sqrt{1 - \cos \theta}.
\end{align*}
We can see that $h^{\prime \prime}$ is negative everywhere, as desired.
\end{proof}

\begin{thm}\label{Minimizing frustration reduces spanning tree}
Let $G = (V, E, \sigma)$ be a connected magnetic graph. Let $c:V \rightarrow S^1$ be a switching function that minimizes the quantity $\sum_{u \sim v} 
\abs*{\overline{c_u} \sigma_{uv} c_v - 1}$. Then there is some spanning tree in $G$ such that $c$ switches all the signatures in the tree to 1.
\end{thm}
\begin{proof}
Let the switched version of $G$ be called $H$. Suppose for contradiction that there is no spanning tree in $H$ consisting of all 1-edges. Let $S \subseteq V$ contain as many vertices as possible while being spanned (in $H$) by a tree of all 1-edges. Since not all of $H$ is spanned by such a tree, we have $S \neq V$. Thus, there is at least one edge crossing the cut $(S, V \setminus S)$. And since $S$ contains as many vertices as possible, none of the edges crossing that cut have signature 1. Let the signatures of the crossing edges (going from $S$ to $V \setminus S$) be labeled $\tau_1, \dots, \tau_k$.

Now define the function $f : \R \rightarrow \R$ by $$f(\theta) := \sum_{j=1}^k \abs*{e^{i \theta} \tau_j - 1}.$$ In fact, we have 
$$f(\theta) = \sum_{j=1}^k \abs*{e^{i (\theta + \arg \tau_j)} - 1} = \sum_{j=1}^k g(\theta + \arg \tau_j),$$
where $g : \R \rightarrow \R$ is given by $$g(\theta) := \abs*{e^{i \theta} - 1} = \sqrt{\sin^2{\theta} + (\cos \theta - 1)^2} = \sqrt{2 - 2 \cos \theta}.$$ 
For fixed $j \in \set*{1, \dots, k}$, we have $\tau_j \neq 1$, meaning $\arg \tau_j \neq 0$. Thus, by \cref{lemma for frustration spanning tree thm}, $g$ is twice-differentiable at $\arg \tau_j$ and $g^{\prime \prime}(\arg \tau_j) < 0$. Therefore, $f$ is twice-differentiable at 0 and 
$$
f^{\prime \prime}(0) = \sum_{j=1}^k g^{\prime \prime}(\arg \tau_j) < 0.
$$
It follows that $f$ does not have a minimum at 0, so there exists some $\alpha \in \R$ with $f(\alpha) < f(0)$. That is, 
\begin{equation}\label{eqn:frustration ineq}
\sum_{j=1}^k \abs*{e^{i \alpha} \tau_j - 1} < \sum_{j=1}^k \abs*{\tau_j - 1}.
\end{equation}

Now, define a new switching function $b$ for $G$ by 
$$
b_v :=
\begin{cases}
c_v &\mbox{if} \quad v \in S \\
e^{i \alpha} c_v &\mbox{if} \quad v \notin S 
\end{cases}.
$$
We claim that 
$$\sum_{u \sim v}\abs*{\overline{b_u} \sigma_{uv} b_v - 1} < \sum_{u \sim v}\abs*{\overline{c_u} \sigma_{uv} c_v - 1},$$
which will contradict the optimality of $c$. If $(u, v)$ is an edge that doesn't cross the cut $(S, V \setminus S)$, then we have 
$$
\abs*{\overline{b_u} \sigma_{uv} b_v - 1} = \abs*{\overline{c_u} \sigma_{uv} c_v - 1}.
$$
On the other hand, for the edges that do cross the cut we have 
\begin{align*}
\sum_{u \in S, v \notin S} \abs*{\overline{b_u} \sigma_{uv} b_v - 1}
&= \sum_{u \in S, v \notin S} \abs*{\overline{c_u} \sigma_{uv} \left(e^{i \alpha} c_v \right) - 1} \\
&= \sum_{u \in S, v \notin S} \abs*{e^{i \alpha} \left( \overline{c_u} \sigma_{uv} c_v \right) - 1} \\
&< \sum_{u \in S, v \notin S} \abs*{\overline{c_u} \sigma_{uv} c_v - 1} && \text{(by \cref{eqn:frustration ineq})}.
\end{align*}
\end{proof}

\pagebreak
\section{Magnetic Graphs as Group Elements}
\begin{defn}
Let $G$ be a magnetic graph whose signatures are all roots of unity. The \textit{multiplicative order} of $G$ is the smallest $k \in \N$ such that raising all signatures to the power of $k$ makes $G$ balanced.
\end{defn}

There are a few different ways to characterize this idea:

\begin{prop}\label{characterizations of multiplicative order}
Let $G$ be a magnetic graph whose signatures are all roots of unity, and let $k \in \N$.  Then the following are equivalent.
\begin{enumerate}
\item $G$ has multiplicative order $k$.
\item $S^\prime_k$ is the subgroup of $S^\prime$ generated by the cycle products of $G$.
\item $k$ is the smallest natural number such that the cycle products of $G$ all belong to $S^\prime_k$. 
\end{enumerate}
\end{prop}

From 3, it is clear that we have:

\begin{prop}
Multiplicative order is invariant under switching.
\end{prop}

\begin{defn}
A connected magnetic graph $G$ is called \textit{quasi-simple} if there is a spanning tree $S$ such that the signatures along $S$ are all equal to 1. We may also be more specific and say that $G$ is quasi-simple along $S$.
\end{defn}

Quasi-simple graphs are useful to us because of the following properties:



\begin{prop}
Let $G$ be a connected magnetic graph, and let $S$ be a spanning tree for $G$. Then there is a unique magnetic graph which is switching equivalent to $G$ and quasi-simple along $S$.
\end{prop}

\pagebreak
\section{Lifts}

We want a way to convert a magnetic graph into a simple graph while preserving most of its important information. Unfortunately, there is no way to do this in general, since there are too many possible signatures that may appear in a magnetic graph. However, if the signatures are roots of unity (in other words, if they all belong to some $S^\prime_k$), then we can use a lift.

\begin{defn}
Let $k \in \N$, and let $G = (V, E, \sigma)$ be a magnetic graph whose signatures all belong to $S^\prime_k$. Then the \textit{$k$-lift} of $G$ is the simple graph on the vertices $V \times S^\prime_k$, where $(u, z) \sim (v, \omega)$ exactly when $u \sim v$ and $z \sigma_{uv} = \omega$. 
\end{defn}

When looking at a lift, we can consider just the vertices of the form $(v, \omega_0)$ for some fixed $\omega_0$, or we can consider just the vertices of the form $(v_0, \omega)$ for some fixed $v_0$. These are called levels and fibers, respectively. 

\begin{defn}
Let $H$ be the $k$-lift of a magnetic graph $G$, and let $\omega \in S^\prime_k$. Then \textit{level $\omega$} of $H$ is the subgraph of $H$ whose vertices have the form $(v, \omega)$ for some $v \in V(G)$. 
\end{defn}

\begin{defn}
Let $H$ be the $k$-lift of a magnetic graph $G$, and let $v \in S^\prime_k$. Then \textit{fiber $v$} of $H$ is the subgraph of $H$ whose vertices have the form $(v, \omega)$ for some $\omega \in S^\prime_k$. 
\end{defn}

Lifts interact nicely with connected components and switching equivalence, as the next two facts show.

\begin{prop}\label{lifts of connected components}
Let $G$ be a magnetic graph whose signatures all belong to $S^\prime_k$. Then the \textit{$k$-lift} of $G$ is isomorphic to the union of the $k$-lifts of its connected components.
\end{prop}
\begin{proof}
This follows easily from the definition of $k$-lift.
\end{proof}

\begin{thm}\label{switching equivalent graphs have isomorphic lifts}
Let $G^\sigma=(V, E, \sigma)$ and $G^\tau=(V, E, \tau)$ be magnetic graphs on the same vertices and edges, where all signatures belong to $S^\prime_k$. If $G^\sigma$ and $G^\tau$ are switching equivalent, then their $k$-lifts are isomorphic.
\end{thm}
\begin{proof}
Because of \cref{lifts of connected components} and \cref{switching equivalence of connected components}, we may assume $G^\sigma$ and $G^\tau$ are connected. Now suppose $G^\sigma$ and $G^\tau$ are switching equivalent. Let $H^\sigma$ and $H^\tau$ be the $k$-lifts of $G^\sigma$ and $G^\tau$. We seek an isomorphism $\phi: V(H^\sigma) \rightarrow V(H^\tau)$. To construct this isomorphism, first define a new magnetic graph $G^s = (V, E, s)$, where the signatures $s$ are given by 
$$s_{ij} = \sigma_{ij}\overline{\tau_{ij}}$$
for all vertices $i \sim j$. Then $G^s$ is balanced (since $G^\sigma$ and $G^\tau$ are switching equivalent), so we may refer to sigdist$^s(i, j)$ for any vertices $i, j \in V$. Now fix a vertex $w \in V$, and define our isomorphism $\phi: V(H^\sigma) \rightarrow V(H^\tau)$ by
$$
\phi(v, z) := (v, z \text{ sigdist}^s(v, w))
$$
for all $v \in V$ and $z \in S^\prime_k$.

To show that $\phi$ is indeed an isomorphism, we must first show that it is bijective. But its domain and its codomain are the same finite set, so it suffices to show that it's injective. Let $(v_1, z_1), (v_2, z_2) \in V(H^\sigma)$, and assume 
$$\phi(v_1, z_1) = \phi(v_2, z_2).$$ 
From the definition of $\phi$, it follows that $v_1 = v_2$ and 
$$z_1 \text{ sigdist}^s (v_1, w) = z_2 \text{ sigdist}^s (v_2, w).$$
Substituting $v_2$ for $v_1$ and dividing away the sigdist, the previous equation becomes $z_1 = z_2$. So we have $(v_1, z_1) = (v_2, z_2)$, and we have proven that $\phi$ is injective. 

It remains to show that two vertices are adjacent in $H^\sigma$ if and only if their images under $\phi$ are adjacent in $H^\tau$. To that end, let $(v_1, z_1)$ and $(v_2, z_2)$ be vertices of $H^\sigma$. If $v_1 \not\sim v_2$, then neither $(v_1, z_1) \sim (v_2, z_2) \text{ in } H^\sigma$ nor $\phi(v_1, z_1) \sim \phi(v_2, z_2) \text{ in } H^\tau$, so the desired equivalence holds. On the other hand, assume $v_1 \sim v_2$. Then we have 
\begin{alignat*}{2}
& & &(v_1, z_1) \sim (v_2, z_2) \text{ in } H^\sigma \\
&\iff\quad & &z_1 \sigma_{v_1 v_2} = z_2 \\
&\iff & &z_1 (\sigma_{v_1 v_2} \overline{\tau_{v_1 v_2}}) \tau_{v_1 v_2} = z_2 \\
&\iff & &z_1 s_{v_1 v_2} \tau_{v_1 v_2} = z_2 \\
&\iff & &z_1 s_{v_1 v_2} \left( \text{sigdist}^s (v_2, w) \right) \tau_{v_1 v_2} = z_2 \left( \text{sigdist}^s (v_2, w) \right) \\
&\iff & &z_1 \text{ sigdist}^s (v_1, w) \tau_{v_1 v_2} = z_2 \text{ sigdist}^s (v_2, w) \\
&\iff & &(v_1, z_1 \text{ sigdist}^s (v_1, w)) \sim (v_2, z_2 \text{ sigdist}^s (v_2, w)) \text{ in } H^\tau \\
&\iff & &\phi(v_1, z_1) \sim \phi(v_2, z_2) \text{ in } H^\tau.
\end{alignat*}
\end{proof}

What happens if we apply an unnecessarily large lift to a graph? For example, what if we take the 6-lift of a graph that has a 2-lift? The answer is that we get multiple copies of the smaller lift.

\begin{thm}\label{unnecessarily large lifts}
Let $G$ be a magnetic graph whose signatures all belong to $S^\prime_k$, and let $m \in \N$. Then the $mk$-lift of $G$ is isomorphic to $m$ copies of the $k$-lift of $G$. 
\end{thm}
\begin{proof}
Let $\omega_0, \dots, \omega_{mk-1}$ be the $mk$th roots of unity, starting with 1 and going counterclockwise. Let $J$ be the $k$-lift of $G$ and let $H$ be the $mk$-lift of $G$. For $i \in \set*{0, \dots, m-1}$, let $J_i$ be the subgraph of $H$ consisting of all the levels $\omega_l$ with $l \equiv i \pmod{m}$. In fact, each $J_i$ is made up of $k$ different levels of $H$. 

It is easy to see that the $J_i$'s are disjoint and cover all the vertices of $H$. Also, we claim that the $J_i$'s are mutually disconnected. It suffices to show that every edge of $H$ begins and ends in the same $J_i$. Take an arbitrary edge of $H$; say that it goes from $J_a$ to $J_b$. That means the edge connects a level $\omega_\alpha$ to a level $\omega_\beta$, where $\alpha \equiv a \pmod{m}$ and $\beta \equiv b \pmod{m}$. So the edge corresponds to an edge in $G$ whose signature is $\omega_{\beta-\alpha}$ (assuming  without loss of generality that $\beta \geq \alpha$). All the signatures of $G$ belong to $S^\prime_k$, so in particular $\omega_{\beta-\alpha} \in S^\prime_k$. But $S^\prime_k$ is exactly the set $\set*{\omega_0, \omega_m, \omega_{2m}, \dots, \omega_{(k-1)m}}$, so we must have $m \mid \beta-\alpha$. It follows that $\alpha \equiv \beta \pmod{m}$, so $a \equiv b \pmod{m}$, so $a = b$, as desired.

It remains to show that each $J_i$ is isomorphic to $J$. Fix $i \in \set*{0, \dots, m-1}$. Note that every vertex in $J_i$ is of the form $(v, \omega_{am+i})$ for some $v \in V(G)$ and unique $a \in \set*{0, \dots, k-1}$. So we may define our isomorphism $\phi: V(J_i) \rightarrow V(J)$ by 
$$
\phi(v, \omega_{am+i}) := (v, \omega_{am}).
$$ 
Since $\omega_{am} \in S^\prime_k$, $\phi$ is indeed a function from $V(J_i)$ into $V(J)$. In fact, $\phi$ is clearly surjective, and thus bijective. 

We must finally show that two vertices are adjacent in $J_i$ if and only if their images under $\phi$ are adjacent in $J$. To that end, let $(u, \omega_{pm + i})$ and $(v, \omega_{qm + i})$ be arbitrary vertices of $J_i$. If $u \not\sim v$, then neither $(u, \omega_{pm + i}) \sim (v, \omega_{qm + i})$ in $J_i$, nor $\phi (u, \omega_{pm + i}) \sim \phi (v, \omega_{qm + i})$ in $J$, so the desired equivalence holds. On the other hand, suppose $u \sim v$. Then, letting $\sigma$ be the signature function of $G$, we have
\begin{alignat*}{2}
& & &(u, \omega_{pm + i}) \sim (v, \omega_{qm + i}) \text{ in } J_i \\
&\iff\quad & &\omega_{pm + i} \sigma_{uv} = \omega_{qm + i} \\
&\iff & &\omega_{pm} \omega_i \sigma_{uv} = \omega_{qm} \omega_i \\
&\iff & &\omega_{pm} \sigma_{uv} = \omega_{qm} \\
&\iff & &(u, \omega_{pm}) \sim (v, \omega_{qm}) \text{ in } J \\
&\iff & &\phi (u, \omega_{pm + i}) \sim \phi (v, \omega_{qm + i}) \text{ in } J.
\end{alignat*}
\end{proof}

We can now describe the lift of a balanced graph.

\begin{prop}
Let $G=(V, E, \sigma)$ be a balanced graph with all signatures in $S^\prime_k$. Then the $k$-lift of $G$ is isomorphic to $k$ copies of the simple graph $(V, E)$.
\end{prop}
\begin{proof}
By \cref{balanced iff switching equivalent to simple graph}, we can apply a switching function to $G$ to make it simple. \cref{switching equivalent graphs have isomorphic lifts} says that this does not affect $G$'s $k$-lift, up to isomorphism. But now $G$ has a 1-lift, which is just the simple graph $(V, E)$. By \cref{unnecessarily large lifts}, the $k$-lift of $G$ is isomorphic to $k$ copies of the 1-lift of $G$, so we are done.
\end{proof}

Given a magnetic graph $G$, it would be great if we could find a $k \in \N$ such that 
\begin{enumerate}
\item $G$ (or some switching equivalent graph) can be $k$-lifted; and
\item Whenever $G$ has an $l$-lift, $k \mid l$.
\end{enumerate}
If this were the case, then every lift of $G$ would be an $mk$-lift of $G$ for some $m$, so it could be described as $m$ copies of the $k$-lift. In fact, choosing $k$ to be the multiplicative order of $G$ gives these properties, as we will now show.

\begin{prop}
Let $G$ be a magnetic graph with multiplicative order $k$. Then there is a switching function for $G$ which takes all its signatures into $S^\prime_k$.
\end{prop}
\begin{proof}
We may assume $G$ is connected. Pick a spanning tree $S$ for $G$, and apply the switching function which reduces $G$ along $S$. Then all the edges in $S$ get signature 1, which belongs to $S^\prime_k$, as desired. It remains to consider an arbitrary edge $d \notin S$. Since $S$ is a spanning tree, $d$ makes a cycle with the edges of the $S$. And since $k$ is the multiplicative order of $G$ (this did not change when we applied our switching function), raising each signature in that cycle to the power of $k$ makes the product along the cycle 1. But all the edges in the cycle have signature 1 except for (possibly) $d$, so raising $d$'s signature to the power of $k$ must give 1. In other words, $d \in S^\prime_k$.
\end{proof}

This establishes that if $k$ is the multiplicative order of $G$, then there is a graph which is switching equivalent to $G$ that can be $k$-lifted. The next proposition shows that (thanks to \cref{unnecessarily large lifts}) every lift is made up of one or more copies of this ``atomic" $k$-lift.

\begin{prop}\label{multiplicative order divides all lift sizes}
Let $G$ be a magnetic graph with multiplicative order $k$. Then whenever $G$ has an $l$-lift, $k \mid l$.
\end{prop}
\begin{proof}
Since $G$ has an $l$-lift, all the signatures in $G$ belong to $S^\prime_l$. Thus, viewing $G$ as a group element, $G^l=e$. But any power of a group element that takes it to the identity must be divisible by the order of that element, so $k \mid l$.
\end{proof}

\cref{multiplicative order divides all lift sizes} shows that, when $k$ is the multiplicative order of $G$, a $k$-lift has no redundancy in the sense that it is not composed of multiple copies of a smaller lift (and in fact, there is no smaller lift). But we can go further and say that if $G$ is connected, then the $k$-lift is connected too.

\begin{thm}
Let $G$ be a connected graph with multiplicative order $k$, with all signatures in $S^\prime_k$. Then the $k$-lift of $G$ is connected.
\end{thm}
\begin{proof}
Reduce $G$ along a spanning tree $S$. This does not affect the multiplicative order, the fact that all the signatures are in $S^\prime_k$, or the $k$-lift (up to isomorphism). Call the $k$-lift of the reduced graph $H$. Since $G$ has all signatures 1 along $S$, each level of $H$ is spanned by a copy of $S$, so the levels are internally connected. It remains to show that every level can be reached from every other level. We will show that every level can be reached from level 1. Let $\omega \in S^\prime_k$, and consider level $\omega$. Finding a path from level 1 to level $\omega$ in $H$ means finding a path in $G$ whose signatures multiply to $\omega$. Since $k$ is the multiplicative order of $G$, part 2 of \cref{characterizations of multiplicative order} says there exists a series of cycle products $p_1, \dots, p_m$ in $G$ (not necessarily distinct) which multiply to $\omega$. Walking each of these cycles once and otherwise traveling along $S$ gives a path whose signatures multiply to $\omega$. 
\end{proof}

We finish this section by completely describing the spectrum of a graph's $k$-lift.

\begin{thm}
Let $G$ be a magnetic graph with signatures in $S^\prime_k$, and let $H$ be the $k$-lift of $G$. Then the spectrum of $H$ is the joined spectra of the graphs $G, G^2, \dots, G^k$. 
\end{thm}
\begin{proof}
Let $n$ be the number of vertices of $G$, and let $\omega_0, \dots, \omega_{k-1}$ be the $k$th roots of unity, starting at 1 and going counterclockwise. For each $i \in \set*{1, \dots, k}$, let $\lambda^i_1, \dots, \lambda^i_n$ be the eigenvalues for the graph $G^i$, and let $v^i_1, \dots, v^i_n$ be corresponding nonzero eigenvectors. We have thus defined $nk$ eigenvectors. We will be done if we can convert them to eigenvectors for $H$ (corresponding to the same eigenvalues) and show that the converted eigenvectors are linearly independent.

We now describe the procedure to convert an eigenvector $v^i_{j}$ for $G^i$ to an eigenvector $x^i_{j}$ for $H$. Begin by magnetizing the edges of $H$ so that every edge from level $\omega_a$ to $\omega_b$ (with $a \leq b$) has signature $\omega_{b-a}^i$ in that direction. What we are really doing is applying the switching function whose value at each vertex in level $\omega$ is $\omega^i$. Now define $y^i_j$ to be $k$ concatenated copies of the eigenvector $v^i_j$ (one for each level of $H$). If we treat $y^i_j$ as a list of vertex values on the magnetized version of $H$, then each level of $H$ exactly resembles $G^i$ with the vertex values $v^i_j$. More precisely, any vertex $(u, \omega)$ in the magnetized $H$ has exactly the same value, neighboring signatures, and neighboring vertex values as the vertex $u$ in $G^i$. It follows that the Laplacian of the magnetized $H$ maps the value of $(u, \omega)$ to the same place that the Laplacian of $G^i$ maps the value of $u$ (see \cref{Laplacian times vector}). But we know that the Laplacian of $G^i$ scales each entry of $v^i_j$ by $\lambda^i_j$, so the Laplacian of the magnetized $H$ does the same to each entry of $y^i_j$. In other words, $y^i_j$ is an eigenvector for the magnetized $H$ corresponding to $\lambda^i_j$. Finally, using \cref{similar matrices have same spectrum}, we can convert $y^i_j$ to an eigenvector $x^i_j$ of the unmagnetized $H$ by multiplying by the matrix
$$
\text{diag}(\omega_0^i, \dots, \omega_0^i, \omega_1^i, \dots, \omega_1^i, \dots, \omega_{k-1}^i, \dots, \omega_{k-1}^i)^{-1},
$$
which is equal to
$$
\text{diag}(1, \dots, 1, \omega_{k-1}^i, \dots, \omega_{k-1}^i, \dots, \omega_{1}^i, \dots, \omega_{1}^i).
$$
Carrying out that multiplication gives us our eigenvector 
$$
x_j^i := \begin{bmatrix}
           v_j^i \\
           \omega_{k-1}^i v_j^i \\
           \vdots \\
           \omega_{1}^i v_j^i
         \end{bmatrix}.
$$

It remains to show that the $x_j^i$'s are linearly independent. Observe that the vectors 
$$
\begin{bmatrix}
           1 \\
           \omega_{k-1} \\
           \vdots \\
           \omega_{1}
         \end{bmatrix},
\begin{bmatrix}
           1 \\
           \omega_{k-1}^2 \\
           \vdots \\
           \omega_{1}^2
         \end{bmatrix},       
\dots,
\begin{bmatrix}
           1 \\
           \omega_{k-1}^k \\
           \vdots \\
           \omega_{1}^k
         \end{bmatrix}
=
\begin{bmatrix}
           1 \\
           1 \\
           \vdots \\
           1
         \end{bmatrix}
$$
are the columns of a Vandermonde matrix (move the last vector all the way to the left). Since all the entries in the first vector are distinct, the determinant of that Vandermonde matrix is nonzero. Thus, those vectors are linearly independent. But also the vectors $$v^i_1, \dots, v^i_n$$ are linearly independent, as they were chosen to be linearly independent eigenvectors of $G^i$. Since each 
$$
x_j^i = 
\begin{bmatrix}
           1 \\
           \omega_{k-1}^i \\
           \vdots \\
           \omega_{1}^i
         \end{bmatrix}
\otimes v^i_j
$$
(where $\otimes$ is the tensor product), and the left hand factors are linearly  //TODO
\end{proof}

\section{Operator Norms}

\begin{defn}\label{defn operator norm}
Let $A$ be an $n \times n$ matrix over $\C$, and let $\norm{\cdot}$ be a vector norm on $\C^n$. Then the \textit{operator norm of $A$ induced by $\norm{\cdot}$} is
$$\norm{A} := \sup_{\substack{\norm{x} = 1}} \norm{A x}.$$
We will write $\norm{A}_p$ for the operator norm induced by the $l_p$ vector norm.
\end{defn}

If we're working with some operator norm and then use an undefined vector norm, it can be assumed that the vector norm is the one that induces the operator norm. For example, in the next proposition the vector norm $\norm{\cdot}$ induces the operator norm $\norm{\cdot}$.

\begin{prop}\label{operator norm of matrix-vector product}
Let $A \in M_{n \times n}(\C)$, let $x \in \C^n$, and let $\norm{\cdot}$ be an operator norm. Then $$\norm*{Ax} \leq \norm*{A}\norm*{x}.$$
\end{prop}
\begin{proof}
This is trivially true if $x = 0$. On the other hand, if $x \neq 0$ we have
$$
\norm*{Ax} 
= \frac{1}{\norm{x}} \norm*{Ax} \norm*{x}
= \norm*{A \frac{x}{\norm{x}}} \norm*{x}
\leq \left( \sup_{\norm{v} = 1} \norm{A v} \right) \norm{x}
= \norm{A} \norm{x}.
$$
\end{proof}

\begin{prop}\label{operator norm of matrix power}
Let $M \in M_{n \times n} \C$ and let $\norm{\cdot}$ be an operator norm. Then for any $k \in \N$, we have $$\norm*{M^k} \leq \norm*{M}^k.$$
\end{prop}
\begin{proof}
If $x \in \C^n$ and $\norm{x} = 1$, then
\begin{align*}
\norm*{M^k x} &= \norm*{M\left( M^{k-1} x \right)} \\
&\leq \norm*{M} \norm*{M^{k-1} x} && \text{(by \cref{operator norm of matrix-vector product})} \\
&\leq \norm*{M}^2 \norm*{M^{k-2} x} \\
&\quad\quad \vdots \\
&\leq \norm*{M}^k \norm*{x} \\
&= \norm*{M}^k.
\end{align*}
Therefore, $\norm*{M}^k$ is an upper bound for the set $\set*{\norm*{M^k x} : x \in \C^n \text{ and } \norm{x} = 1}$. But $\norm*{M^k}$ is by definition the least upper bound for that set, so $\norm*{M^k} \leq \norm*{M}^k$.
\end{proof}

\begin{thm}\label{inverse of I - M}
Let $M \in M_{n \times n} \C$, and assume $\norm{M} < 1$ for some operator norm $\norm{\cdot}$. Then $I - M$ is invertible, and
$$
(I - M)^{-1} = \sum_{k=0}^\infty M^k.
$$
\end{thm}
\begin{proof}
If we can show that $\sum_{k=0}^\infty M^k$ converges, then we will have 
\begin{align*}
(I - M) \left( \sum_{k=0}^\infty M^k \right) 
&= \sum_{k=0}^\infty (I - M) M^k \\
&= \sum_{k=0}^\infty M^k - M^{k+1} \\
&= \sum_{k=0}^\infty M^k - \sum_{k=0}^\infty M^{k+1} \\
&= \sum_{k=0}^\infty M^k - \sum_{k=1}^\infty M^{k} \\
&= I,
\end{align*}
and similarly $$\left( \sum_{k=0}^\infty M^k \right) (I-M) = I.$$
So it remains to show $\sum_{k=0}^\infty M^k$ converges. We will show that the sequence of partial sums is Cauchy. To that end, let $\epsilon > 0$ be given. Since $\norm{M} < 1$, we have that $\sum_{k=0}^\infty \norm{M}^k$ is Cauchy. Therefore, there exists $N \in \N$ such that whenever $m > j > N$ we have 
$$
\norm{M}^{j+1} + \norm{M}^{j+2} \dots + \norm{M}^{m} < \epsilon.
$$
So in fact, whenever $m > j > N$ we have 
\begin{align*}
\norm*{\sum_{k=0}^m M^k - \sum_{k=0}^j M^k}
&= \norm*{M^{j+1} + M^{j+2} + \dots + M^{m}} \\
&\leq \norm*{M^{j+1}} + \norm*{M^{j+2}} + \dots + \norm*{M^{m}} \\
&\leq \norm{M}^{j+1} + \norm{M}^{j+2} + \dots + \norm{M}^{m} && \text{(by \cref{operator norm of matrix power})} \\
&< \epsilon.
\end{align*}
\end{proof}
 
\pagebreak
 
\section{Magnetic PageRank}

\begin{defn}\label{defn PageRank matrix}
Let $G=(V, E, \sigma)$ be a magnetic graph. Then $G$'s \textit{PageRank matrix} is the matrix $W$ given by
$$
W_{ij} = 
\begin{cases}
\frac{\sigma_{ij}}{d_j} &\text{ if } i \sim j \\
0 &\text{ if } i \not\sim j
\end{cases}
$$
for all $i, j \in V$.
\end{defn}

In the case where $G$ is simple, this definition aligns with the classical definition for the PageRank matrix.

\begin{prop}\label{operator norm of PageRank matrix leq 1}
For any PageRank matrix $W$, we have $\norm{W}_1 \leq 1$.
\end{prop}
\begin{proof}
Let $n$ be the number of rows/columns in $W$. It suffices to show that for any $x \in \C^n$ with $\norm{x}_1 = 1$, we have $\norm{W x}_1 \leq 1$. Indeed, letting $w_{\bullet j}$ stand for the $j$th column vector of $W$, we have
\begin{align*}
\norm{W x}_1 &= \norm*{\sum_{j=1}^n x_j w_{\bullet j}}_1 \\
&\leq \sum_{j=1}^n \norm*{x_j w_{\bullet j}}_1 \\
&= \sum_{j=1}^n \left( \abs*{x_j} \norm*{w_{\bullet j}}_1 \right) \\
&= \sum_{j=1}^n \left( \abs*{x_j} \sum_{i=1}^n \abs*{w_{i j}} \right) \\
&= \sum_{j=1}^n \left( \abs*{x_j} \sum_{i \sim j} \abs*{\frac{\sigma_{ij}}{d_j}} \right) \\
&= \sum_{j=1}^n \left( \abs*{x_j} \frac{1}{d_j} \sum_{i \sim j} \abs*{\sigma_{ij}} \right) 
= \sum_{j=1}^n \left( \abs*{x_j} \frac{1}{d_j} d_j \right) 
= \sum_{j=1}^n \abs*{x_j} 
= \norm*{x}_1 
= 1.
\end{align*}
\end{proof}

This proposition implies that, like classical PageRank matrices, magnetic PageRank matrices never have an eigenvalue whose absolute value is greater than 1. This could lead us to hope that the eigenvectors of magnetic PageRank matrices share other traits with the eigenvectors of classical PageRank matrices. For example, classical PageRank matrices are associated with a PageRank vector, which is defined to be a certain nonzero vector $x$ satisfying $W x = x$. Unfortunately, such a vector usually doesn't exist for magnetic PageRank matrices, as the next two facts show.

\begin{prop}\label{Laplacian Eigenvectors vs PageRank Eigenvectors}
Let $G$ be a magnetic graph on $n$ vertices, with PageRank matrix $W$ and Laplacian $L$. Let $x \in \C^n$. Then $W^T x = x$ if and only if $L^T x = 0$.
\end{prop}
\begin{proof}
Let $A$ and $D$ be the adjacency matrix and degree matrix of $G$, respectively.  Then $W = A^T D^{-1}$ and $L = D - A$. So it follows that
\begin{align*}
W^T x = x &\iff (A D^{-1})^T x = x \\
&\iff D^{-1} A^T x = x \\
&\iff A^T x = D x \\
&\iff (D - A^T) x = 0 \\
&\iff (D - A)^T x = 0 \\
&\iff L^T x = 0.
\end{align*}
\end{proof}

\begin{prop}\label{traditional PageRank vector exists iff graph has balanced connected component}
Let $G$ be a magnetic graph on $n$ vertices with PageRank matrix $W$. Then there exists a nonzero vector $x \in \C^n$ satisfying $W x = x$ if and only if $G$ has a balanced connected component.
\end{prop}
\begin{proof}
Let $L$ be the Laplacian of $G$. Then,
\begin{alignat*}{3}
&& &\text{There exists a nonzero vector } x \in \C^n \text{ satisfying } W x = x \\
&\iff\quad &&W \text{ has 1 as an eigenvalue} \\
&\iff &&W^T \text{ has 1 as an eigenvalue} \\
&\iff &&L^T \text{ has 0 as an eigenvalue} &\quad &\text{(by \cref{Laplacian Eigenvectors vs PageRank Eigenvectors})} \\
&\iff &&L \text{ has 0 as an eigenvalue} \\
&\iff &&\text{G has a balanced connected component} &&\text{(by \cref{nullity of Laplacian})}.
\end{alignat*}
\end{proof}

\cref{traditional PageRank vector exists iff graph has balanced connected component} dashes our hopes of borrowing the nonmagnetic definition of the PageRank vector. But it turns out we \textit{can} borrow the nonmagnetic definition of the ``personalized" PageRank vector. Our definition is based closely off the one from Chung and Zhao's paper.

\begin{defn}
Suppose we are given a magnetic graph $G$ with PageRank matrix $W$. Then the graph's \textit{personalized PageRank vector} with \textit{jumping constant} $\alpha \in (0, 1)$ and \textit{seed vector} $s \in \C^n$, written pr$(\alpha, s)$, is defined to be the unique $x \in \C^n$ satisfying
$$
\alpha s + (1 - \alpha) W x = x.
$$
\end{defn}

To show that this definition is valid, we need to show that there is indeed a unique $x \in \C^n$ satisfying that equation. The equation can be rewritten as 
\begin{equation}
\alpha s = (I - (1 - \alpha)W)x.\label{eq: PageRank rearranged}
\end{equation}
So it suffices to show $(I - (1 - \alpha)W)$ is invertible. That is the same as saying 1 is not an eigenvalue of the matrix $(1 - \alpha)W$, which is clearly true since 
$$
\norm{W}_1 \leq 1 \implies \norm{(1 - \alpha)W}_1 < 1.
$$ 
This is maybe the most straightforward way to see that our definition of the personalized PageRank vector is valid, but we can alternatively cite \cref{inverse of I - M}, which says that $(I - (1 - \alpha)W)$ is invertible and 
$$
(I - (1 - \alpha)W)^{-1} = \sum_{k=0}^{\infty} ((1 - \alpha) W)^k = \sum_{k=0}^{\infty} (1 - \alpha)^k W^k.
$$ This approach is nice because it lets us actually solve \cref{eq: PageRank rearranged} for the PageRank vector $x$. Multiplying both sides of \cref{eq: PageRank rearranged} by $(I - (1 - \alpha)W)^{-1}$ gives the following fact.

\begin{prop}
Let $W$ be an $n \times n$ PageRank matrix, $\alpha \in (0, 1)$, and $s \in \C^n$. Then $$\pr(\alpha, s) = \alpha \left( \sum_{k=0}^{\infty} (1 - \alpha)^k W^k \right) s.$$
\end{prop}

We can also define a ``lazy" variant of the PageRank matrix, which has most of the same properties as the standard PageRank matrix but is sometimes easier to work with.

\begin{defn}
Let $G$ be a magnetic graph with PageRank matrix $W$. Then the graph's \textit{lazy PageRank matrix} is $Z := (I + W)/2$.
\end{defn}

If we can show that $\norm{Z}_1 \leq 1$, then $Z$ will be associated with a personalized PageRank vector just like $W$. Indeed, we have
\begin{align*}
\norm{Z}_1 &= \norm{(I+W)/2}_1 \\
&\leq \frac{1}{2} \norm{I}_1 + \frac{1}{2} \norm{W}_1 \\
&\leq \frac{1}{2} (1) + \frac{1}{2} (1) \\
&= 1.
\end{align*}
So we can make the following defintion:

\begin{defn}
Suppose we are given a magnetic graph on $n$ vertices with lazy PageRank matrix $Z$. Then the graph's \textit{lazy personalized PageRank vector} with \textit{jumping constant} $\alpha \in (0, 1)$ and \textit{seed vector} $s \in \C^n$, written $\lazypr (\alpha, s)$, is defined to be the unique $x \in \C^n$ satisfying
$$
\alpha s + (1 - \alpha) Z x = x.
$$
\end{defn}

\pagebreak

\section{Magnetic PageRank and the Discrete Green's Function}

Throughout this section, let $G=(V, E, \sigma)$ be a magnetic graph on $n$ vertices, with degree matrix $D$ and adjacency matrix $A$. Assume $G$ is connected and has at least 2 vertices, so that $D$ is invertible. Let $G$ have Laplacian $L$, PageRank matrix $W$, and lazy PageRank matrix $Z$. Fix $\alpha \in (0, 1)$, and let $$\beta := \frac{2 \alpha}{1 - \alpha}.$$
\begin{defn} 
The \textit{normalized Laplacian} of $G$ is
$$
\lap := D^{-1/2} L D^{-1/2}. 
$$
\end{defn}
\begin{defn}
The \textit{normalized adjacency matrix} of $G$ is
$$
\normadj := D^{-1/2} A D^{-1/2}. 
$$
\end{defn}

Since $L$ and $A$ are Hermitian, $\lap$ and $\normadj$ are Hermitian too. Normalizing the Laplacian and adjacency matrix in this way is nice, because instead of the unnormalized relationship $L = D - A$, we have the cleaner relationship $$\lap = I - \normadj.$$
But what does this have to do with PageRank? It turns out that $$W = D^{1/2} \normadj D^{-1/2},$$ so $W$ can be viewed as an ``asymmetric" (or more precisely non-Hermitian) version of $\normadj$. This idea of working with asymmetric versions of Hermitian matrices will show up a lot. It's important because it bridges the gap from the asymmetric PageRank world of $W$ and $Z$ to the symmetric Laplacian world of $\normadj$ and $\lap$.

Because $\lap$ is Hermitian, it has $n$ real eigenvalues. Going forward we will refer to these as $$\lambda_1 \leq \dots \leq \lambda_n$$ with the corresponding orthonormal eigenvectors $$\phi_1, \dots, \phi_n.$$

\begin{prop}
The eigenvalues of $\lap$ satisfy $$0 \leq \lambda_1 \leq \dots \leq \lambda_n \leq 2.$$
\end{prop}
\begin{proof}
We have $\normadj = I - \lap$, so $\normadj$ has the same eigenvectors $\phi_1, \dots, \phi_n$ as $\lap$, but with eigenvalues $$1 - \lambda_1, \dots, 1 - \lambda_n.$$ Since $W$ is similar to $\normadj$, it has those eigenvalues too. But from \cref{operator norm of PageRank matrix leq 1} we have $\norm{W}_1 \leq 1$, so those eigenvalues have absolute value at most 1. That is, for every $k \in \set{1, \dots, n}$, we have
$$
\abs*{1 - \lambda_k} \leq 1 \implies 0 \leq \lambda_k \leq 2.
$$
\end{proof}

In order to define Green's Function, we must first introduce a variant of the normalized Laplacian.

\begin{defn}
The \textit{$\beta$-normalized Laplacian} is $$\lap_\beta := \beta I + \lap.$$
\end{defn}

We can see $\lap_\beta$ shares the eigenvectors $\phi_1, \dots, \phi_n$ with $\lap$, except the corresponding eigenvalues of $\lap_\beta$ are $\beta + \lambda_1, \dots, \beta + \lambda_n$. Since $\beta > 0$, we can conclude that $\lap_\beta$ does not have 0 as an eigenvalue, and is therefore invertible. So we can make the following definition.

\begin{defn}
The \textit{discrete Green's function} is $$\green_\beta := \lap_\beta^{-1}.$$
\end{defn}

Since the inverse of an invertible Hermitian matrix is itself Hermitian, we have that $\green_\beta$ is Hermitian.

Next we want to define an asymmetric version of the Green's Function. Asymmetrizing the Green's function will help us relate it to PageRank, as we discussed earlier. We asymmetrize the Green's function the same way we asymmetrized $\normadj$ to get $W$.

\begin{defn}
The \textit{asymmetric discrete Green's function} is
$$
\asymgreen_\beta := D^{1/2} \green_\beta D^{-1/2}.
$$
\end{defn}

There is another way to arrive at this definition. We could follow the same procedure we used to define $\green_\beta$, where we start with $\lap$, add $\beta I$, and then take the inverse; except instead we start with an asymmetric version of $\lap$. The asymmetric version of $\lap$ we need is the following.

\begin{defn}
The \textit{discrete Laplace operator} is the matrix
$$
\laplace := D^{1/2} \lap D^{-1/2}.
$$
\end{defn}

If we were to define $\asymgreen_\beta$ using $\laplace$, taking the same approach as for $\green_\beta$, we would want to say $$\asymgreen_\beta = \left( \beta I + \laplace \right)^{-1}.$$ Indeed, this definition is equivalent to ours, as the next proposition shows.

\begin{prop}\label{Asymmetric Green's in terms of Laplace operator}
$\beta I + \laplace$ is invertible, and $$\asymgreen_\beta = \left( \beta I + \laplace \right)^{-1}.$$
\end{prop}
\begin{proof}
Note that $\laplace$ has $n$ nonnegative eigenvalues because it is similar to $\lap$. Since $\beta > 0$, it follows that $\beta I + \laplace$ has $n$ nonzero eigenvalues, meaning $\beta I + \laplace$ is invertible. Finally, we have
\begin{align*}
\left( \beta I + \laplace \right)^{-1}
&= \left( \beta I + D^{1/2} \lap D^{-1/2} \right)^{-1} \\
&= \left(D^{1/2} \left( \beta I + \lap \right) D^{-1/2} \right)^{-1} \\
&= \left(D^{1/2} \lap_\beta D^{-1/2} \right)^{-1} \\
&= D^{1/2} \green_\beta D^{-1/2} \\
&= \asymgreen_\beta.
\end{align*}
\end{proof}

Since $\laplace$ is an asymmetric version of $\lap$ and $W$ is an asymmetric version of $\normadj$, we naturally have the following relationship.

\begin{prop}\label{Laplace operator and PageRank}
We have: $$\laplace = I - W.$$
\end{prop}
\begin{proof}
$$
\laplace 
= D^{1/2} \lap D^{-1/2} 
= D^{1/2} (I - \normadj) D^{-1/2}
= I - D^{1/2} \normadj D^{-1/2}
= I - W.
$$
\end{proof}

Now we are finally ready to relate PageRank to $\asymgreen_\beta$.

\begin{thm}
For any seed vector $s \in \C^n$, we have
$$
\lazypr (\alpha, s) = \beta \asymgreen_\beta s.
$$
\end{thm}
\begin{proof}
\begin{align*}
\frac{\lazypr (\alpha, s)}{\beta} 
&= \frac{\alpha}{\beta} \left[ I - (1 - \alpha)Z \right]^{-1} s \\ 
&= \left( \frac{\beta}{\alpha} I - 2 Z \right)^{-1} s \\
&= \left[ \left( \beta + 2 \right) I - 2 Z \right]^{-1} s \\
&= \left[ \left( \beta + 2 \right) I - (I + W) \right]^{-1} s \\
&= \left[ \beta I + (I - W) \right]^{-1} s \\
&= \left( \beta I + \laplace \right)^{-1} s &&\text{(by \cref{Laplace operator and PageRank})} \\
&= \asymgreen_\beta s &&\text{(by \cref{Asymmetric Green's in terms of Laplace operator})}.
\end{align*}
\end{proof}

We now turn our attention to the PageRank vectors resulting from seed vectors of a special form. Namely, let $S$ be a nonempty induced subgraph of $G$, and define the seed vector $f_S \in \C^n$ by
$$
(f_S)_v =
\begin{cases}
\frac{d_v}{\vol(S)} &\text{if } v \in S \\
0 &\text{otherwise}
\end{cases}.
$$
Equivalently, we have $$f_S = D \chi_S / \vol (S),$$
where $\chi_S \in \C^n$ is 1 at the vertices in $S$ and 0 everywhere else. Our goal is to prove the following theorem, wherein $\lazypr(\alpha, f_S)(S)$ denotes $\sum_{v \in S} \lazypr(\alpha, f_S)_v$.

\begin{thm}\label{bound on lazy PageRank}
$\lazypr(\alpha, f_S)(S)$ is real, and 
$$
\lazypr(\alpha, f_S)(S) \geq 1 - \frac{\abs*{\partial(S)} + \sum_{(u,v) \in E^{or}(S)}{(1 - \Re(\sigma_{uv}))}}{\beta \vol (S)}.
$$
\end{thm}

Before we can prove this theorem, we will need several facts about positive-semidefinite matrices. These were mostly taken from Wikipedia, Math Stack Exchange, and Linear Algebra Done Right.

\begin{defn}
A matrix $B \in M_{n \times n}(\C)$ is said to be \textit{positive-semidefinite} if $B$ is Hermitian and $$\inner*{ Bv, v } \geq 0$$ for every $v \in \C^n$. (Note that $\inner*{ Bv, v }$ is automatically real since $B$ is Hermitian.)
\end{defn}

\begin{lem}\label{eigenvalue characterization of positive-semidefiniteness}
Let $B \in M_{n \times n}(\C)$. Then $B$ is positive-semidefinite iff $B$ is Hermitian and all the eigenvalues of $B$ are nonnegative.
\end{lem}
\begin{proof}
This is part of Proposition 7.35 on page 226 of Linear Algebra Done Right (3rd Edition).
\end{proof}

\begin{lem}\label{eigenvalue characterization of Hermitian matrix}
Let $B \in M_{n \times n}(\C)$. If there exists an orthonormal basis for $\C^n$ made up of eigenvectors of $B$, and their eigenvalues are all real, then $B$ is Hermitian.
\end{lem}

\begin{lem}\label{eigenvalue condition for positive semidefiniteness}
Let $B \in M_{n \times n}(\C)$. If there exists an orthonormal basis for $\C^n$ made up of eigenvectors of $B$, and their eigenvalues are all real and nonnegative, then $B$ is positive-semidefinite. 
\end{lem}
\begin{proof}
This follows from \cref{eigenvalue characterization of Hermitian matrix} and \cref{eigenvalue characterization of positive-semidefiniteness}.
\end{proof}

\begin{lem}\label{HBH positive semidefinite}
Let $B, H \in M_{n \times n}(\C)$. Assume $B$ is positive-semidefinite and $H$ is Hermitian. Then $HBH$ is positive-semidefinite.
\end{lem}
\begin{proof}
To see that $HBH$ is Hermitian, observe that
$$
(HBH)^* = H^* B^* H^* = HBH,
$$
where $B^* = B$ because $B$ is Hermitian. Now let $v \in \C^n$ be arbitrary. Then,
\begin{align*}
\inner*{HBH v, v}
&= \inner*{B H v, H v} &&\text{(because $H$ is Hermitian)} \\
&= \inner*{B (H v), (H v)} \\
&\geq 0 &&\text{(because $B$ is positive-semidefinite)}.
\end{align*}
Therefore, $HBH$ is positive-semidefinite.
\end{proof}

The next two lemmas will provide steps in the proof of \cref{bound on lazy PageRank}.

\begin{lem}\label{Laplacian expression is positive semidefinite}
$L - \beta \laplace \asymgreen_\beta D$ is positive-semidefinite.
\end{lem}
\begin{proof}
We have 
\begin{align*}
L - \beta \laplace \asymgreen_\beta D
&= L - \beta \left( L D^{-1} \right) \left( D^{1/2} \green_\beta D^{-1/2} \right) D \\
&= L - \beta L D^{-1/2} \lap_\beta^{-1} D^{1/2} \\
&= D^{1/2} \left( \lap - \beta \lap \lap_\beta^{-1} \right) D^{1/2}.
\end{align*}
By \cref{HBH positive semidefinite}, since $D^{1/2}$ is Hermitian it suffices to show $\lap - \beta \lap \lap_\beta^{-1}$ is positive-semidefinite. Recall that $\phi_1, \dots, \phi_n$ are orthonormal eigenvectors for $\lap$, with corresponding eigenvalues $0 \leq \lambda_1 \leq \dots \leq \lambda_n.$ We claim that for every $k \in \set{1, \dots, n}$, $\phi_k$ is an eigenvector of $\lap - \beta \lap \lap_\beta^{-1}$ with a nonnegative, real eigenvalue. If we can show this, then by \cref{eigenvalue condition for positive semidefiniteness}, the proof will be finished. For fixed $k$, we have:
\begin{align*}
\left( \lap - \beta \lap \lap_\beta^{-1} \right) \phi_k 
&= \lap \phi_k - \beta \lap \lap_\beta^{-1} \phi_k \\
&= \lambda_k \phi_k - \frac{\beta}{\beta + \lambda_k} \lap \phi_k \\
&= \lambda_k \phi_k - \frac{\beta \lambda_k}{\beta + \lambda_k} \phi_k \\
&= \left(\lambda_k - \frac{\beta \lambda_k}{\beta + \lambda_k}  \right) \phi_k \\
&= \left( \frac{\lambda_k^2}{\beta + \lambda_k} \right) \phi_k.
\end{align*}
We see that $\phi_k$ is an eigenvector of $\lap - \beta \lap \lap_\beta^{-1}$, and the eigenvalue is real and nonnegative, as desired.
\end{proof}

\begin{lem}
We have:
$$
\beta \asymgreen_\beta = I - \laplace \asymgreen_\beta
$$
\end{lem}
\begin{proof}
\begin{align*}
\beta \asymgreen_\beta &=
\left[ (\beta I + \laplace) - \laplace \right] \asymgreen_\beta \\
&= \left( \asymgreen_\beta^{-1} - \laplace \right) \asymgreen_\beta &&\text{(by \cref{Asymmetric Green's in terms of Laplace operator})} \\
&= I - \laplace \asymgreen_\beta.
\end{align*}
\end{proof}

We are finally ready to prove \cref{bound on lazy PageRank}.

\begin{proof}[Proof of \cref{bound on lazy PageRank}]
We have:
\begin{align}
\lazypr(\alpha, f_S)(S)
&= \chi_S^* \lazypr(\alpha, f_S) \nonumber \\
&= \chi_S^* \beta \asymgreen_\beta f_S \nonumber \\
&= \chi_S^* (I - \laplace \asymgreen_\beta) f_S \nonumber \\
&= 1 - \chi_S^* \laplace \asymgreen_\beta f_S \nonumber \\
&= 1 - \frac{1}{\beta \vol(S)} \chi_S^* \left( \beta \laplace \asymgreen_\beta D \right) \chi_S \label{pre leqstep} \\
&\geq 1 - \frac{1}{\beta \vol(S)} \chi_S^* L \chi_S \label{leqstep} \\
&= 1 - \frac{\vol{(S)} - \sum_{(u,v) \in E^{or}(S)} \sigma_{uv}}{\beta \vol(S)} \label{sum over L step} \\
&= 1 - \frac{\vol{(S)} - \abs*{E^{or}(S)} + \abs*{E^{or}(S)} - \sum_{(u,v) \in E^{or}(S)} \sigma_{uv}}{\beta \vol(S)} \nonumber \\
&= 1 - \frac{\abs*{\partial(S)} + \sum_{(u,v) \in E^{or}(S)}{(1 - \sigma_{uv})}}{\beta \vol (S)} \nonumber \\
&= 1 - \frac{\abs*{\partial(S)} + \sum_{(u,v) \in E^{or}(S)}{(1 - \Re(\sigma_{uv}))}}{\beta \vol (S)}. \nonumber
\end{align}

Combining \cref{Laplacian expression is positive semidefinite} with the fact that $L$ is Hermitian, we get that $\beta \laplace \asymgreen_\beta D$ is Hermitian, so \cref{pre leqstep} is real. This proves that $\lazypr(\alpha, f_S)(S)$ is real, as claimed in the theorem. 

We get \cref{leqstep} by observing that both $\chi_S^* L \chi_S$ and $\chi_S^* \left( \beta \laplace \asymgreen_\beta D \right) \chi_S$ are real (since the inner matrices are Hermitian), and then using \cref{Laplacian expression is positive semidefinite} to conclude that
\begin{align*}
\chi_S^* L \chi_S - \chi_S^* \left( \beta \laplace \asymgreen_\beta D \right) \chi_S
&= \chi_S^* \left( L - \beta \laplace \asymgreen_\beta D \right) \chi_S \\
&= \inner*{\left( L - \beta \laplace \asymgreen_\beta D \right) \chi_S, \chi_S} \\
&\geq 0.
\end{align*}

To get \cref{sum over L step} we note that taking $\chi_S^* L \chi_S$ gives the sum of the entries in $L$ restricted to the submatrix with indices in $S$.
\end{proof}

We will next interpret \cref{bound on lazy PageRank} in terms of Cheeger ratios, giving a more direct magnetic analogue of Chung's Section 5, Lemma 2 in her paper \textit{PageRank as a discrete Green's function}. First, we have to define a magnetic version of the Cheeger ratio. Recall that $\iota$ in this definition is the frustration index from \cref{defn: frustration index}.

\begin{defn}
Let $S$ be an induced subgraph of $G$, with $S \neq 0$ and $S \neq G$. Then the \textit{Cheeger ratio} of $S$ is denoted $h(S)$ and given by
$$
h(S) := \frac{\abs*{\partial (S)} + \iota(S)}{\min \set*{\vol(S), \vol(G) - \vol(S)}}.
$$
\end{defn}

In the following theorem, $\lazypr_c$ denotes the lazy PageRank vector for the graph $G$, but after $G$ has been switched by the switching function $c$.

\begin{thm}
Let $S$ be an induced subgraph of $G$, with $S \neq 0$ and $S \neq G$. Then,
$$
\max_{c} \lazypr_c (\alpha, f_S) (S) \geq 1 - \frac{1}{\beta} h(S)
$$
where the max is taken over switching functions $c: V(G) \rightarrow S^1$.
\end{thm}
\begin{proof}
The left hand side is real by \cref{bound on lazy PageRank}, so the inequality makes sense. We have:
\begin{align*}
\max_{c} \lazypr_c (\alpha, f_S) (S)
&\geq \max_{c} \left( 1 - \frac{\abs*{\partial(S)} + \sum_{(u,v) \in E^{or}(S)}{(1 - \Re(\overline{c_u} \sigma_{uv} c_v))}}{\beta \vol (S)} \right) && \text{(by \cref{bound on lazy PageRank})} \\
& \geq \max_{c} \left( 1 - \frac{\abs*{\partial(S)} + \sum_{(u,v) \in E^{or}(S)}{\abs*{1 - \overline{c_u} \sigma_{uv} c_v}}}{\beta \vol (S)} \right) \\
&= 1 - \frac{\abs*{\partial(S)} + \min\limits_c \sum_{(u,v) \in E^{or}(S)}{\abs*{1 - \overline{c_u} \sigma_{uv} c_v}}}{\beta \vol (S)} \\
&= 1 - \frac{\abs*{\partial(S)} + \iota(S)}{\beta \vol (S)} \\
&\geq 1 - \frac{1}{\beta} h(S).
\end{align*}
Note that throughout this proof, the maximums/minimums exist by a compactness argument.
\end{proof}

\section{Effective Resistance and Hitting Time}

We will continue using the variables and assumptions from the previous section. However,
for this section we will also need a more explicit understanding of the vector $\phi_1$ in the case that $G$ is balanced. For that purpose, we define $l_1 \in \C^n$ by 
$$
l_1 := \frac{\sqrt{n}}{\norm*{D^{-1/2} \phi_1}} D^{-1/2} \phi_1. 
$$
Then if $G$ is balanced, $l_1$ allows us to nicely describe $\phi_1$ per the following lemma.

\begin{lem}
Suppose $G$ is balanced. Then,
\begin{enumerate}
\item $l_1 \in \ker L$
\item Every entry of $l_1$ belongs to $S^1$
\item For any vertex $v$ of $G$, we have 
$$
\phi_1(v) = \frac{\sqrt{d_v}}{\sqrt{\vol(G)}} l_1(v).
$$
\end{enumerate}
\end{lem}
\begin{proof}
Since $G$ is balanced, $\lap$ has 0 as an eigenvalue, so $\lap \phi_1 = 0.$ Therefore,
$$
D^{-1/2} L l_1 = \frac{\sqrt{n}}{\norm*{D^{-1/2} \phi_1}} \lap \phi_1 = 0,
$$
so $l_1 \in \ker L$ as desired. By \cref{characterization of null space of Laplacian}, all the entries of $l_1$ have the same modulus. But $\norm{l_1} = \sqrt{n}$, so in fact all the entries of $l_1$ have modulus 1, as desired. Finally, observe that for any vertex $v$ of $G$, we have
$$
\phi_1(v)
= \chi_v^* \phi_1
= \chi_v^* \frac{D^{1/2} l_1}{\norm{D^{1/2} l_1}} 
= \chi_v^* \frac{D^{1/2} l_1}{\sqrt{\vol(G)}}
= \frac{\sqrt{d_v}}{\sqrt{\vol(G)}} \chi_v^* l_1
= \frac{\sqrt{d_v}}{\sqrt{\vol(G)}} l_1(v).
$$
\end{proof}

To define the effective resistance, we first need to define the unnormalized discrete Green's function $\green$.

\begin{defn}
The \textit{unnormalized discrete Green's function} is 
$$
\green := \sum_{\substack{1 \leq i \leq n \\ \lambda_i \neq 0}} \frac{1}{\lambda_i} \phi_i \phi_i^*.
$$
\end{defn}

We can view $\green$ as a ``pseudoinverse" of $\lap$. If our graph $G$ is unbalanced, meaning $\lap$ is invertible, then $\green = \lap^{-1}$. On the other hand, if $G$ is balanced, meaning $\lap$ has nullity 1, then $\green$ acts like an inverse of $\lap$ in the orthogonal complement of $\ker \lap$, but still takes the vectors in $\ker \lap$ to 0. Thinking along these lines, we can see that $\green$ is independent of the choice of $\phi_i$'s, so our definition is valid for any graph.

We can see that $\green$ has the same eigenvectors $\phi_1, \dots, \phi_n$ as $\lap$, and all its eigenvalues are real and nonnegative. Thus, by \cref{eigenvalue condition for positive semidefiniteness}, $\green$ is positive-semidefinite. We are now ready to define the effective resistance.

\begin{defn}
Let $u$ and $v$ be vertices of $G$. Then the \textit{effective resistance} between $u$ and $v$ is 
$$
R(u, v) := (\chi_v - \chi_u)^* D^{-1/2} \green D^{-1/2} (\chi_v - \chi_u).
$$
\end{defn}

Since $\green$ is positive-semidefinite, \cref{HBH positive semidefinite} gives that $D^{-1/2} \green D^{-1/2}$ is positive-semidefinite. Thus, $R(u, v)$ is real and nonnegative for any vertices $u$ and $v$.

We can also define a generalized version of the effective resistance, which is related to the parameter $\alpha$.

\begin{defn}
Let $u$ and $v$ be vertices of $G$. Then the \textit{generalized effective resistance} between $u$ and $v$ is 
$$
R_\alpha(u, v) := \beta (\chi_v - \chi_u)^* D^{-1/2} \green_\beta D^{-1/2} (\chi_v - \chi_u).
$$
\end{defn}

Similarly to the effective resistance, this generalized effective resistance is always real and nonnegative. An advantage of the generalized effective resistance is that it can be written in terms of PageRank.

\begin{prop}
For any vertices $u$ and $v$, we have $$R_\alpha(u, v) = \frac{\lazypr(\alpha, \chi_v)(v)}{d_v} - \frac{\lazypr(\alpha, \chi_v)(u)}{d_u} + \frac{\lazypr(\alpha, \chi_u)(u)}{d_u} - \frac{\lazypr(\alpha, \chi_u)(v)}{d_v}.$$
\end{prop}
\begin{proof}
\begin{align*}
R_\alpha(u, v) &= \beta (\chi_v - \chi_u)^* D^{-1/2} \green_\beta D^{-1/2} (\chi_v - \chi_u) \\
&= \beta (\chi_v - \chi_u)^* D^{-1} \asymgreen_\beta (\chi_v - \chi_u) \\
&= \left( \chi_v^* D^{-1} - \chi_u^* D^{-1} \right) \left[ \beta \asymgreen_\beta \chi_v - \beta \asymgreen_\beta \chi_u \right] \\
&= \left( \frac{1}{d_v} \chi_v^* - \frac{1}{d_u} \chi_u^* \right) \left[ \lazypr(\alpha, \chi_v) - \lazypr(\alpha, \chi_u) \right] \\
&= \frac{\lazypr(\alpha, \chi_v)(v)}{d_v} - \frac{\lazypr(\alpha, \chi_v)(u)}{d_u} + \frac{\lazypr(\alpha, \chi_u)(u)}{d_u} - \frac{\lazypr(\alpha, \chi_u)(v)}{d_v}.
\end{align*}
\end{proof}

The first and third terms in that sum are real by \cref{bound on lazy PageRank}. Since the whole sum is real, we may conclude that the second term plus the fourth term is real too. However, we can prove an even stronger statement:

\begin{prop}
For any vertices $u$ and $v$, we have
$$
\overline{\lazypr(\alpha, \chi_v)(u)} = \frac{d_u}{d_v} \lazypr(\alpha, \chi_u)(v).
$$
\end{prop}
\begin{proof}
\begin{align*}
\overline{\lazypr(\alpha, \chi_v)(u)} &= \overline{\chi_u^* \lazypr(\alpha, \chi_v)} \\
&= \left( \chi_u^* \beta \asymgreen_\beta \chi_v \right)^* \\
&= \beta \chi_v^* \asymgreen_\beta^* \chi_u \\
&= \beta \chi_v^* \left( D^{1/2} \green_\beta D^{-1/2} \right)^* \chi_u \\
&= \beta \chi_v^* D^{-1/2} \green_\beta D^{1/2} \chi_u \\
&= \beta \chi_v^* D^{-1} \asymgreen_\beta D \chi_u \\
&= \beta \left( \frac{1}{d_v} \chi_v^* \right) \asymgreen_\beta (d_u \chi_u) \\
&= \frac{d_u}{d_v} \chi_v^* \beta \asymgreen_\beta \chi_u \\
&= \frac{d_u}{d_v} \lazypr(\alpha, \chi_u)(v).
\end{align*}
\end{proof}

The next theorem bounds the effective resistance using the generalized effective resistance. Since the generalized effective resistance can be expressed in terms of PageRank, this allows us to estimate the effective resistance using PageRank.

\begin{thm}
If $G$ is balanced, then we have
$$
\abs*{\beta R(u, v) - R_\alpha (u, v)}
\leq \frac{\abs*{1 - \sigdist(u, v)}^2}{\vol(G)} + \frac{\beta^2}{\lambda_2^2}\left( \frac{1}{d_u} + \frac{1}{d_v} \right).
$$
And if $G$ is unbalanced, then
$$
\abs*{\beta R(u, v) - R_\alpha (u, v)}
\leq \frac{\beta^2}{\lambda_1^2}\left( \frac{1}{d_u} + \frac{1}{d_v} \right).
$$
\end{thm}

The effective resistance is related to another quantity called the hitting time.

\begin{defn}
Given two vertices $u$ and $v$, the \textit{hitting time} from $u$ to $v$ is 
$$
H(u, v) := \vol(G) (\chi_v - \chi_u)^* D^{-1/2} \green D^{-1/2} \chi_v.
$$  
\end{defn}

\begin{prop}
We have $$\vol(G) R(u, v) = H(u, v) + H(v, u).$$
\end{prop}
\begin{proof}
This follows easily from the definitions.
\end{proof}

It is fairly easy to check that switching functions interact nicely with the effective resistance and hitting time. The key to the nice relationships is the fact that switching functions interact nicely with Green's function, as the next lemma shows.

\begin{lem}
Suppose that a switching function $c: V \rightarrow S^\prime$ is applied to $G$ to obtain a new graph $G^\prime$. Let $\green^\prime$ be the Green's function for $G^\prime$. Let $C$ be the diagonal matrix whose entries are given by $C_{vv} = c_v$. Then, $$\green^\prime = C^* \green C.$$
\end{lem}
\begin{proof}
Recall that $\phi_1, \dots, \phi_n$ are orthonormal eigenvectors for $\lap$, with corresponding eigenvalues $\lambda_1, \dots, \lambda_n$. The Laplacian for $G^\prime$ is $\lap^\prime := C^* \lap C$, so the list $C^* \phi_1, \dots, C^* \phi_n$ is made up of eigenvectors of $\lap^\prime$, and the eigenvalues are still $\lambda_1, \dots, \lambda_n$. It is easy to check that these eigenvectors have norm 1. Moreover, for every $1 \leq i < j \leq n$ we have
$$
\inner*{C^* \phi_i, C^* \phi_j} = \inner*{\phi_i, C C^* \phi_j} = \inner*{\phi_i, I \phi_j} = \inner*{\phi_i, \phi_j} = 0.
$$
So the eigenvectors are actually orthnormal. Thus, by the definition of Green's function we have
$$
G^\prime = \sum_{\substack{1 \leq i \leq n \\ \lambda_i \neq 0}} \frac{1}{\lambda_i} \left( C^* \phi_i \right) \left( C^* \phi_i \right)^*
= \sum_{\substack{1 \leq i \leq n \\ \lambda_i \neq 0}} \frac{1}{\lambda_i} C^* \phi_i \phi_i^* C
= C^* \left( \sum_{\substack{1 \leq i \leq n \\ \lambda_i \neq 0}} \frac{1}{\lambda_i} \phi_i \phi_i^* \right) C
= C^* \green C.
$$
\end{proof}

With this lemma, it is easy to check the following series of statements.

\begin{prop}
Suppose that a switching function $c: V \rightarrow S^\prime$ is applied to $G$ to obtain a new graph $G^\prime$. Let $R^c$ be the effective resistance function for $G^\prime$. Then for any vertices $u$ and $v$, we have $$R^c(u, v) = (c_v \chi_v - c_u \chi_u)^* D^{-1/2} \green D^{-1/2} (c_v \chi_v - c_u \chi_u).$$
\end{prop}

\begin{prop}
Suppose that a switching function $c: V \rightarrow S^\prime$ is applied to $G$ to obtain a new graph $G^\prime$. Let $H^c$ be the hitting time function for $G^\prime$. Then for any vertices $u$ and $v$, we have $$H^c(u, v) = (c_v \chi_v - c_u \chi_u)^* D^{-1/2} \green D^{-1/2} (c_v \chi_v).$$
\end{prop}

\begin{prop}
If $u, v$ are vertices of $G$ and $b, c$ are switching functions for $G$ with $\overline{b_u}b_v = \overline{c_u}c_v$, then $R^b (u, v) = R^c (u, v)$ and $H^b (u, v) = H^c (u, v)$.
\end{prop}

\begin{corollary}
If $u, v$ are vertices of $G$ and $c$ is a switching function for $G$ with $c_u = c_v$, then $R^c (u, v) = R(u, v)$ and $H^c (u, v) = H(u, v)$.  
\end{corollary}

To state the next theorem nicely, we will make a new definition.   
\begin{defn}
For a connected magnetic graph $G$, the \textit{average signature distance} between two vertices $u$ and $v$ is
$$
\asd(u, v) := \begin{cases}
\sigdist(u, v) &\mbox{if $G$ is balanced} \\
0 &\mbox{otherwise}
\end{cases}.
$$
\end{defn}
Why is it called the average signature distance? When we walk along a path from $u$ to $v$, we accumulate a product of signatures. Choosing a different path may result in a different product. Informally, the average signature distance is the (unweighted) average of all the unique products we can get by choosing different paths.

\begin{thm}
Let $u$ and $v$ be distinct vertices in $G$. Then,
$$
H(u, v) = \asd(u, v) + \frac{1}{d_u} \sum_{w \sim u} H^w(w, v)
$$
where $H^w(w, v)$ denotes the hitting time from $w$ to $v$, computed on a switched version of $G$ where the switching function $c$ satisfies $\overline{c_w}c_v = \sigma_{uw}$.
\end{thm}
\begin{proof}
We have (justifications for numbered steps are below):
\begin{align}
\frac{H(u, v)}{\vol(G)} 
&= (\chi_v - \chi_u)^* D^{-1/2} \green D^{-1/2} \chi_v \nonumber \\
&= \chi_v^* D^{-1/2} \green D^{-1/2} \chi_v - \chi_u^* D^{-1/2} \green D^{-1/2} \chi_v \nonumber \\
&= \chi_v^* D^{-1/2} \green D^{-1/2} \chi_v + \chi_u^* D^{-1/2} I D^{-1/2} \chi_v - \chi_u^* D^{-1/2} \green D^{-1/2} \chi_v \nonumber \\
&= \chi_v^* D^{-1/2} \green D^{-1/2} \chi_v + \chi_u^* D^{-1/2} (I -\lap \green) D^{-1/2} \chi_v - \chi_u^* D^{-1/2} (I - \lap) \green D^{-1/2} \chi_v \nonumber \\
&= \frac{\asd(u, v)}{\vol(G)} + \chi_v^* D^{-1/2} \green D^{-1/2} \chi_v - \chi_u^* D^{-1/2} (I - \lap) \green D^{-1/2} \chi_v \label{eqn:lap and green to asd} \\
&= \frac{\asd(u, v)}{\vol(G)} + \chi_v^* D^{-1/2} \green D^{-1/2} \chi_v - \chi_u^* D^{-1/2} \normadj \green D^{-1/2} \chi_v \nonumber \\
&= \frac{\asd(u, v)}{\vol(G)} + \chi_v^* D^{-1/2} \green D^{-1/2} \chi_v - \chi_u^* D^{-1} A D^{-1/2} \green D^{-1/2} \chi_v \nonumber \\
&= \frac{\asd(u, v)}{\vol(G)} + \chi_v^* D^{-1/2} \green D^{-1/2} \chi_v - \frac{1}{d_u} \chi_u^* A D^{-1/2} \green D^{-1/2} \chi_v \nonumber \\
&= \frac{\asd(u, v)}{\vol(G)} + \chi_v^* D^{-1/2} \green D^{-1/2} \chi_v - \frac{1}{d_u} \sum_{w \sim u} \sigma_{uw} \chi_w^* D^{-1/2} \green D^{-1/2} \chi_v \label{eqn:adj to sum} \\
&= \frac{\asd(u, v)}{\vol(G)} + \frac{1}{d_u} \left( \sum_{w \sim u} \chi_v^* D^{-1/2} \green D^{-1/2} \chi_v \right) - \frac{1}{d_u} \sum_{w \sim u} \sigma_{uw} \chi_w^* D^{-1/2} \green D^{-1/2} \chi_v \nonumber \\
&= \frac{\asd(u, v)}{\vol(G)} + \frac{1}{d_u} \sum_{w \sim u} \left( \chi_v^* - \sigma_{uw} \chi_w^* \right) D^{-1/2} \green D^{-1/2} \chi_v \nonumber \\
&= \frac{\asd(u, v)}{\vol(G)} + \frac{1}{d_u} \sum_{w \sim u} \left( \chi_v - \sigma_{wu} \chi_w \right)^* D^{-1/2} \green D^{-1/2} \chi_v \nonumber \\
&= \frac{\asd(u, v)}{\vol(G)} + \frac{1}{d_u} \sum_{w \sim u} \frac{H^w(w, v)}{\vol(G)}. \nonumber
\end{align}

For \cref{eqn:lap and green to asd}, observe that if $G$ is unbalanced, then $\lap \green = I$ and the result follows. Now assume that $G$ is balanced. Then $I - \lap \green = \phi_1 \phi_1^*$, since those matrices act the same way on the basis $\phi_1, \dots, \phi_n$. It follows that
\begin{align*}
\chi_u^* D^{-1/2} (I - \lap \green) D^{-1/2} \chi_v
&= \chi_u^* D^{-1/2} \phi_1 \phi_1^* D^{-1/2} \chi_v \\
&= \frac{1}{\sqrt{d_u d_v}} \chi_u^* \phi_1 \phi_1^* \chi_v \\
&= \frac{1}{\sqrt{d_u d_v}} \phi_1(u) \overline{\phi_1(v)} \\
&= \frac{1}{\sqrt{d_u d_v}} \left( \frac{\sqrt{d_u}}{\sqrt{\vol(G)}} l_1(u) \right) \left( \frac{\sqrt{d_v}}{\sqrt{\vol(G)}} \overline{l_1(v)} \right) \\
&= \frac{l_1(u) \overline{l_1(v)}}{\vol(G)} \\
&= \frac{\sigdist(u, v)}{\vol(G)}.
\end{align*}

For \cref{eqn:adj to sum}, note that for any vector $y \in \C^n$, 
$$
\chi_u^* A y = (A y)_{u} = \sum_{j=1}^n A_{uj} y_j = \sum_{w \sim u} A_{uw} y_w = \sum_{w \sim u} \sigma_{uw} y_w = \sum_{w \sim u} \sigma_{uw} \chi^*_w y.
$$
\end{proof}

\end{document}
